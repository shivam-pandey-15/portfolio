# Research: 2.6 Query Rewriting & Expansion

## The Problem
Users type imperfect queries. Rewriting and expansion bridge the gap between query and corpus.

---

## The Expansion Spectrum

```
No Expansion ←───────────────────────────────────→ Heavy Expansion
High Precision                                      High Recall
High Zero-Result Rate                               Low Precision
"Returns exactly what you asked for"               "Returns anything vaguely related"
```

### When to Use What
| Query Type | Expansion Level | Example |
|------------|-----------------|---------|
| SKU/Product code | None | "XYZ-123-ABC" |
| High-specificity | Light | "Nike Air Max 90 size 11" |
| Category browse | Moderate | "running shoes" |
| Natural language | Heavy | "something comfy for long walks" |
| Zero results | Very heavy + fallback | After initial search fails |

---

## Query Expansion Techniques (Deep Dive)

### 1. Synonym Expansion

#### Building the Synonym Table
```python
class SynonymBuilder:
    def __init__(self):
        self.click_logs = ClickLogStore()
        self.embedding_model = load_model("e5-base")
        
    def build_synonyms_from_clicks(self) -> Dict[str, List[str]]:
        """
        Learn synonyms from user behavior:
        If users search "sneakers" and click products titled "athletic shoes",
        then "sneakers" and "athletic shoes" are synonyms.
        """
        co_occurrence = defaultdict(Counter)
        
        for log in self.click_logs.iterate():
            query_terms = set(tokenize(log.query))
            product_terms = set(tokenize(log.clicked_product.title))
            
            for qt in query_terms:
                for pt in product_terms:
                    if qt != pt and is_similar_pos(qt, pt):  # Same part of speech
                        co_occurrence[qt][pt] += log.weight  # Weight by clicks
        
        # Filter to high-confidence synonyms
        synonyms = {}
        for term, candidates in co_occurrence.items():
            top_candidates = candidates.most_common(10)
            synonyms[term] = [
                c[0] for c in top_candidates 
                if c[1] > MIN_COUNT and self._is_valid_synonym(term, c[0])
            ][:5]
        
        return synonyms
    
    def _is_valid_synonym(self, term1: str, term2: str) -> bool:
        # Embedding similarity check
        emb1 = self.embedding_model.encode(term1)
        emb2 = self.embedding_model.encode(term2)
        similarity = cosine_similarity(emb1, emb2)
        return similarity > 0.75
```

#### Production Synonym Table Examples
| Query Term | Synonyms | Source |
|------------|----------|--------|
| sneakers | athletic shoes, tennis shoes, running shoes | Click data |
| couch | sofa, loveseat, settee | Manual + WordNet |
| laptop | notebook, computer, ultrabook | Click data |
| cheap | affordable, budget, inexpensive | Manual |
| kids | children, boys, girls | Manual |

#### Asymmetric Synonyms
```python
ASYMMETRIC_SYNONYMS = {
    # These work one direction but not the other:
    "puppy": ["dog"],           # OK: puppy → dog results
    # But NOT: "dog": ["puppy"]  # Bad: dog → only puppies
    
    "iPhone 15": ["iPhone"],    # OK: specific → general
    # But NOT: "iPhone": ["iPhone 15"]  # Bad: general → specific
    
    "NYC": ["New York City", "New York"],  # OK: abbreviation → full
    # But NOT: "New York": ["NYC"]  # May not want abbreviation results
}
```

### 2. Stemming & Lemmatization

#### Comparison
| Query | Stemmed (Porter) | Lemmatized | Best For |
|-------|------------------|------------|----------|
| "running shoes" | "run shoe" | "run shoe" | Both work |
| "better laptops" | "better laptop" | "good laptop" | Lemmatization |
| "children's clothing" | "children cloth" | "child clothing" | Lemmatization |
| "studies" | "studi" | "study" | Lemmatization |

#### Production Pattern
```python
class TextNormalizer:
    def __init__(self):
        self.stemmer = PorterStemmer()
        self.lemmatizer = WordNetLemmatizer()
        
    def normalize_for_index(self, text: str) -> str:
        """
        For indexing: use both original and normalized forms.
        """
        tokens = word_tokenize(text)
        
        normalized = []
        for token in tokens:
            normalized.append(token)  # Original
            
            # Add lemma if different
            lemma = self.lemmatizer.lemmatize(token.lower())
            if lemma != token.lower():
                normalized.append(lemma)
        
        return " ".join(normalized)
    
    def normalize_for_query(self, query: str) -> str:
        """
        For querying: more aggressive normalization.
        """
        tokens = word_tokenize(query)
        return " ".join([
            self.lemmatizer.lemmatize(token.lower())
            for token in tokens
        ])
```

### 3. Semantic Expansion (Embeddings)

#### Finding Related Terms
```python
class SemanticExpander:
    def __init__(self):
        self.model = SentenceTransformer("all-MiniLM-L6-v2")
        self.term_embeddings = self._load_term_embeddings()
        self.term_index = self._build_ann_index()
        
    def expand(self, query: str, k: int = 5) -> List[str]:
        """
        Find semantically similar terms using embeddings.
        """
        query_embedding = self.model.encode(query)
        
        # ANN search in term embedding space
        similar_terms = self.term_index.search(query_embedding, k=k*2)
        
        # Filter by similarity threshold
        filtered = [
            term for term, score in similar_terms
            if score > 0.7 and term.lower() != query.lower()
        ]
        
        return filtered[:k]

# Examples:
# "running shoes" → ["jogging shoes", "athletic footwear", "marathon shoes"]
# "comfortable chair" → ["ergonomic chair", "office seating", "desk chair"]
```

### 4. Query Log Mining

#### Learning Reformulations
```python
class ReformulationMiner:
    def mine_reformulations(self, session_logs: List[Session]) -> Dict[str, str]:
        """
        If user searches A, then immediately searches B and clicks,
        A → B might be a valid rewrite.
        """
        reformulations = defaultdict(Counter)
        
        for session in session_logs:
            queries = session.queries
            
            for i in range(len(queries) - 1):
                current = queries[i]
                next_query = queries[i + 1]
                
                # If no click on current, but click on next
                if not current.had_click and next_query.had_click:
                    # This looks like a successful reformulation
                    reformulations[current.text][next_query.text] += 1
        
        # Filter to confident reformulations
        result = {}
        for query, candidates in reformulations.items():
            top = candidates.most_common(1)
            if top and top[0][1] > MIN_OBSERVATIONS:
                result[query] = top[0][0]
        
        return result

# Examples discovered:
# "ipone" → "iPhone"
# "cheep laptop" → "cheap laptop"
# "running sneakers" → "running shoes"
```

---

## Query Rewriting Techniques

### 1. Structured Query Rewriting

#### Entity-to-Filter Conversion
```python
def entities_to_structured_query(query: str, entities: List[Entity]) -> StructuredQuery:
    """
    Convert extracted entities to search filters.
    """
    filters = {}
    text_parts = tokenize(query)
    
    for entity in entities:
        if entity.type == "BRAND":
            filters["brand"] = entity.normalized_value
            text_parts = remove_tokens(text_parts, entity.tokens)
            
        elif entity.type == "SIZE":
            filters["size"] = parse_size(entity.text)
            text_parts = remove_tokens(text_parts, entity.tokens)
            
        elif entity.type == "COLOR":
            filters["color"] = entity.normalized_value
            text_parts = remove_tokens(text_parts, entity.tokens)
            
        elif entity.type == "PRICE":
            price_constraint = parse_price_constraint(entity.text)
            filters.update(price_constraint)
            text_parts = remove_tokens(text_parts, entity.tokens)
    
    return StructuredQuery(
        text_query=" ".join(text_parts),
        filters=filters
    )

# Example:
# Input: "blue nike running shoes size 10 under $100"
# Output:
# StructuredQuery(
#     text_query="running shoes",
#     filters={
#         "color": "blue",
#         "brand": "Nike",
#         "size": "10",
#         "price": {"max": 100}
#     }
# )
```

### 2. Natural Language Rewriting (LLM)

#### LLM Query Understanding
```python
class LLMQueryRewriter:
    def __init__(self):
        self.llm = load_llm("gpt-4-turbo")  # Or smaller local model
        
    def rewrite(self, query: str, context: dict) -> StructuredQuery:
        prompt = f"""
Convert this search query to structured format.

Query: "{query}"
Context: User is on {context.get('platform', 'e-commerce')} site

Return JSON:
{{
    "category": "product category",
    "attributes": ["list", "of", "desired", "attributes"],
    "constraints": {{"price_max": null, "brand": null}},
    "exclude": ["things", "to", "NOT", "show"],
    "intent": "browse|purchase|research"
}}
"""
        
        response = self.llm.generate(prompt)
        parsed = json.loads(response)
        
        return StructuredQuery.from_dict(parsed)

# Example:
# Input: "comfortable shoes for someone who stands all day, not too expensive"
# Output:
# {
#     "category": "comfort shoes",
#     "attributes": ["cushioned", "supportive", "arch support"],
#     "constraints": {"price_max": 150},
#     "exclude": ["heels", "dress shoes"],
#     "intent": "purchase"
# }
```

### 3. Template-Based Rewriting

#### Common Patterns
```python
REWRITE_TEMPLATES = [
    # Price patterns
    {
        "pattern": r"(?P<category>\w+)\s+under\s+\$?(?P<price>\d+)",
        "rewrite": lambda m: StructuredQuery(
            text_query=m.group("category"),
            filters={"price": {"max": int(m.group("price"))}}
        )
    },
    
    # Size patterns
    {
        "pattern": r"(?P<category>\w+)\s+size\s+(?P<size>[\d.]+)",
        "rewrite": lambda m: StructuredQuery(
            text_query=m.group("category"),
            filters={"size": m.group("size")}
        )
    },
    
    # "For X" patterns
    {
        "pattern": r"(?P<category>\w+)\s+for\s+(?P<audience>men|women|kids)",
        "rewrite": lambda m: StructuredQuery(
            text_query=m.group("category"),
            filters={"department": m.group("audience")}
        )
    },
    
    # "Best X" patterns
    {
        "pattern": r"best\s+(?P<category>.+)",
        "rewrite": lambda m: StructuredQuery(
            text_query=m.group("category"),
            sort={"field": "rating", "order": "desc"},
            boosts={"review_count": 1.2}
        )
    }
]
```

---

## Case Studies

### Case Study 1: Amazon's Query Rewriting

**Challenge:** Handle 100M+ unique queries with high accuracy.

**Approach:**
1. **Dictionary-based:** Known brands, categories, attributes
2. **Template matching:** Common patterns (size, color, price)
3. **ML model:** For complex/natural language queries
4. **Fallback:** LLM for tail queries (but cached)

**Results:**
| Approach | Coverage | Precision | Latency |
|----------|----------|-----------|---------|
| Dictionary | 60% | 98% | <1ms |
| Templates | 20% | 95% | <2ms |
| ML Model | 15% | 88% | ~10ms |
| LLM (cached) | 5% | 85% | ~50ms |

### Case Study 2: Google's Query Expansion

**Challenge:** 15% of queries are never-seen-before.

**Approach:**
1. **Synonym expansion:** Curated + learned from clicks
2. **Conceptual expansion:** "car" → "automobile", "vehicle"
3. **Related queries:** "Used by people who also searched..."
4. **Neural expansion:** BERT-based semantic similarity

**Key Learning:** 
- Over-expansion hurts more than under-expansion
- Always show original query results first
- Expanded results should be clearly marked

### Case Study 3: Spotify's Mood Expansion

**Challenge:** "Sad songs" means different things to different people.

**Approach:**
1. **Mood taxonomy:** Map subjective terms to audio features
2. **Personalization:** "Sad" for this user = indie folk, for another = R&B
3. **Temporal expansion:** Consider time/season

**Implementation:**
```python
MOOD_MAPPINGS = {
    "sad": {
        "audio_features": {"valence": [0.0, 0.3], "energy": [0.0, 0.5]},
        "genres": ["indie folk", "acoustic", "slowcore"],
        "expansion_queries": ["melancholy", "heartbreak", "emotional"]
    },
    "happy": {
        "audio_features": {"valence": [0.7, 1.0], "energy": [0.6, 1.0]},
        "genres": ["pop", "dance", "funk"],
        "expansion_queries": ["upbeat", "cheerful", "feel good"]
    }
}
```

---

## Precision vs Recall Trade-offs

### Dynamic Expansion Control
```python
class AdaptiveExpander:
    def expand(self, query: str, initial_results: int) -> ExpansionLevel:
        """
        Adjust expansion based on initial result count.
        """
        if initial_results > 100:
            # Plenty of results, don't expand (preserve precision)
            return ExpansionLevel.NONE
        
        elif initial_results > 20:
            # Good results, light expansion only
            return ExpansionLevel.LIGHT  # Synonyms only
        
        elif initial_results > 5:
            # Few results, moderate expansion
            return ExpansionLevel.MODERATE  # Synonyms + related
        
        elif initial_results > 0:
            # Very few results, heavy expansion
            return ExpansionLevel.HEAVY  # Semantic + LLM
        
        else:
            # Zero results, maximum effort
            return ExpansionLevel.FALLBACK  # Relax constraints + category

# Then adjust weights based on level:
EXPANSION_WEIGHTS = {
    ExpansionLevel.NONE: {"original": 1.0, "synonyms": 0.0, "semantic": 0.0},
    ExpansionLevel.LIGHT: {"original": 1.0, "synonyms": 0.8, "semantic": 0.0},
    ExpansionLevel.MODERATE: {"original": 1.0, "synonyms": 0.8, "semantic": 0.5},
    ExpansionLevel.HEAVY: {"original": 1.0, "synonyms": 0.9, "semantic": 0.8},
    ExpansionLevel.FALLBACK: {"original": 0.5, "synonyms": 1.0, "semantic": 1.0}
}
```

---

## Common Mistakes

### Mistake 1: Over-Expanding Brands
```python
# BAD: "Nike" → "Nike OR Adidas OR Puma"
# User specifically asked for Nike!

# GOOD: Don't expand brand entities
def should_expand(token: str, entities: List[Entity]) -> bool:
    for entity in entities:
        if entity.type == "BRAND" and token in entity.text:
            return False
    return True
```

### Mistake 2: Expanding Price Constraints
```python
# BAD: "under $50" → "under $50 OR under $100 OR under $200"
# User has a budget!

# GOOD: Preserve constraints, only relax as fallback
def relax_constraints(query: StructuredQuery) -> StructuredQuery:
    if query.result_count == 0:
        relaxed = query.copy()
        if relaxed.filters.get("price", {}).get("max"):
            relaxed.filters["price"]["max"] *= 1.2  # Only 20% relaxation
            relaxed.relaxation_note = "Showing items slightly above budget"
        return relaxed
    return query
```

### Mistake 3: Ignoring Negation in Expansion
```python
# Query: "laptop without touchscreen"
# BAD: Expand "touchscreen" → also matches touchscreen laptops

# GOOD: Track negations separately
def expand_with_negation(query: str) -> ExpandedQuery:
    negations = extract_negations(query)  # ["touchscreen"]
    positive_terms = extract_positive_terms(query)  # ["laptop"]
    
    expanded_positive = expand_terms(positive_terms)
    
    return ExpandedQuery(
        must=expanded_positive,
        must_not=negations,  # Never expand negations
        original=query
    )
```

---

## Measuring Expansion Quality

### Key Metrics
| Metric | What It Measures | Target |
|--------|------------------|--------|
| Zero-result rate | Expansion successfully rescues | <5% |
| Precision@K | Expanded results still relevant | >0.8 |
| CTR (expanded vs original) | Users click expanded results | >0.5x original |
| Conversion (expanded) | Expansion leads to purchase | >0.3x original |

### A/B Testing Framework
```python
expansion_experiment = {
    "name": "semantic_expansion_v2",
    "population": "queries with < 20 results",
    "control": {
        "expansion": "synonyms_only"
    },
    "treatment": {
        "expansion": "synonyms + semantic"
    },
    "metrics": [
        "zero_result_rate",
        "ctr",
        "conversion_rate",
        "revenue_per_search"
    ],
    "guardrails": [
        {"metric": "precision_at_10", "min_threshold": 0.75}
    ]
}
```

---

## Key Takeaways

1. **Expansion increases recall** but can hurt precision
2. **Rewriting converts to structured** — better for filtering
3. **Context matters** — "Python" means different things
4. **Don't over-expand** — respect user's specificity
5. **Tiered approach** — expand more as results decrease
6. **Measure everything** — expansion can easily go wrong
7. **Protect entities** — don't expand brands, constraints
8. **Learn from clicks** — best synonyms come from user behavior
