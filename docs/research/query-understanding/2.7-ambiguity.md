# Research: 2.7 Handling Ambiguity

## The Problem
Most queries are ambiguous. The same words mean different things to different users.

---

## Types of Ambiguity

### 1. Lexical Ambiguity
Same word, multiple meanings.

| Query | Possible Meanings |
|-------|-------------------|
| "Apple" | Company, fruit, music, store |
| "Python" | Programming language, snake, Monty Python |
| "Jaguar" | Car, animal, macOS version |
| "Mercury" | Planet, element, car brand, Freddie Mercury |
| "Java" | Programming language, coffee, island |

### 2. Syntactic Ambiguity
Same words, different parsing.

| Query | Interpretation 1 | Interpretation 2 |
|-------|------------------|------------------|
| "small dog food" | Food for small dogs | Small-sized dog food bags |
| "red wine glass" | Glass for red wine | Wine glass that is red |
| "new york pizza" | Pizza from NYC | Pizza that is "New York style" |
| "wireless mouse pad" | Pad for wireless mouse | Wireless charging mouse pad |

### 3. Intent Ambiguity
Unclear what user wants to DO.

| Query | Intent 1 | Intent 2 |
|-------|----------|----------|
| "iPhone 15" | Buy one | Compare specs |
| "Facebook" | Log in | Read news about Facebook |
| "Python" | Download it | Learn it |
| "running" | Running shoes | Running advice |

### 4. Scope Ambiguity
How specific is the user?

| Query | Narrow | Broad |
|-------|--------|-------|
| "laptop" | Wants to buy a specific laptop | Browsing the category |
| "shoes" | Knows what they want | Just exploring |
| "camera" | Ready to purchase | Researching options |

---

## Real-World Examples

### Example 1: "Apple" on Google
Google's solution: **Diversify results**
- Position 1-3: Apple Inc (most likely intent)
- Position 4: Apple fruit (alternative intent)
- Knowledge panel: Apple Inc info
- "People also search for": Related queries

### Example 2: "Running" on Amazon
Amazon's solution: **Category disambiguation**
- Suggests: "Running shoes", "Running shorts", "Running watch"
- Shows facets: Category, Brand, Price
- Personalizes based on past purchases

### Example 3: "Python" on Stack Overflow
Stack Overflow's solution: **Tag-based**
- Results tagged [python] (programming)
- No snake-related results (domain context)
- Context eliminates ambiguity

---

## Disambiguation Strategies

### 1. Result Diversification
Show different interpretations in the results.

```
Query: "Apple"
Results:
  #1: apple.com (company)
  #2: Apple iPhone (product)
  #3: Apple nutrition facts (fruit)
  #4: Apple Music (service)
```

**Pros:** Covers all bases, no wrong answer
**Cons:** Majority doesn't want fruit if searching tech

### 2. Query Clarification
Ask the user what they meant.

```
Query: "Python"
System: "Did you mean: Python programming | Python (snake) | Monty Python"
```

**Pros:** Precise intent capture
**Cons:** Adds friction, slows user down

### 3. Contextual Disambiguation
Use signals to guess intent.

| Signal | How It Helps |
|--------|--------------|
| User history | Previous tech searches → "Python" = programming |
| Device | Searching from IDE → programming |
| Location | Near zoo → maybe the snake |
| Time | Black Friday → shopping intent |
| Referrer | From coding tutorial → programming |

### 4. Entity Linking
Link query to known entities.

```
Query: "Tim Cook"
Entity: Tim Cook (Apple CEO)
Sub-entities: Age, net worth, news
Related: Apple Inc, Steve Jobs
```

### 5. Faceted Navigation
Let user self-disambiguate.

```
Query: "Apple"
Facets: 
  - Category: [Electronics] [Fruit] [Music]
  - Brand: [Apple Inc] [Generic]
```

---

## Signals for Disambiguation

### User Signals
| Signal | Weight | Example |
|--------|--------|---------|
| Past purchases | Very High | Bought iPhone → tech user |
| Search history | High | Searched "Python tutorial" → programmer |
| Clicks | High | Clicked on tech results before |
| Demographics | Medium | Age, occupation (if known) |
| Saved preferences | High | Explicitly set interests |

### Session Signals
| Signal | Example |
|--------|---------|
| Previous query | "laptop specs" then "laptop" → still shopping |
| Page context | Searched from product page |
| Time in session | Just started vs. deep in browsing |
| Cart contents | Has iPhone in cart → Apple = tech |

### Query Signals
| Signal | Example |
|--------|---------|
| Query length | Short = ambiguous, Long = specific |
| Entities detected | Brand name clarifies intent |
| Modifiers | "how to" = informational |
| Suffix | "near me" = local |

---

## Measuring Ambiguity

### Query Entropy
High entropy = high ambiguity

```
"iPhone 15 Pro 256GB" → Low entropy (specific)
"Apple" → High entropy (many meanings)
```

### Click Entropy
If clicks are distributed across many items, query is ambiguous.

```
Query: "laptop"
Clicks: MacBook (20%), Dell (15%), HP (15%), Lenovo (12%)...
→ High click entropy = ambiguous (user exploring)

Query: "MacBook Pro M3"
Clicks: MacBook Pro M3 (80%), MacBook Pro M3 Max (15%)...
→ Low click entropy = specific
```

### Reformulation Rate
High reformulation = query didn't work.

```
User: "Python"
User: "Python programming tutorial"
→ User had to clarify (ambiguity not resolved)
```

---

## Handling Failure Cases

### When You Guess Wrong
```
Query: "Apple"
User wanted: Fruit
System showed: Apple Inc
```

**Solutions:**
- "Not what you're looking for? Try: Apple fruit"
- Show diverse results
- Easy refinement through facets

### When There's No Good Answer
```
Query: "that thing"
→ Too ambiguous to serve
```

**Solutions:**
- Ask for clarification
- Show trending/popular items
- Use session context heavily

---

## Best Practices

### 1. Default to Most Likely Intent
- "Apple" → Apple Inc (90% of searchers want this)
- Use popularity/click data to determine

### 2. But Cover Alternatives
- Show 1-2 results for minority intent
- Or show "Did you mean" links

### 3. Let Users Clarify Easily
- Facets/filters visible
- Autocomplete suggests disambiguated queries
- Quick refinement options

### 4. Use Context Aggressively
- User history is gold
- Session behavior helps
- Device/location matter

### 5. Learn from Behavior
- Track which disambiguation worked
- A/B test strategies
- Update models on reformulation patterns

---

## Key Takeaways

1. **Most queries are ambiguous** — assume ambiguity by default
2. **Context is the best disambiguator** — user history, session, location
3. **Diversify results** for high-ambiguity queries
4. **Don't force users to type more** — provide refinement UI
5. **Learn from clicks and reformulations** — data tells you what users meant
6. **Measure click entropy** — high entropy = need better disambiguation
