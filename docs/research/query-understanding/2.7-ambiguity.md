# Research: 2.7 Handling Ambiguity

## The Problem
Most queries are ambiguous. The same words mean different things to different users.

---

## Ambiguity by the Numbers

### How Ambiguous Are Queries?
| Query Length | % Ambiguous | Example |
|--------------|-------------|---------|
| 1 word | 85% | "apple", "python", "mercury" |
| 2 words | 65% | "running shoes", "best laptop" |
| 3-4 words | 40% | "men's running shoes nike" |
| 5+ words | 15% | "men's nike air max 90 size 11 black" |

**Key insight:** Short queries are the most common AND the most ambiguous.

### Ambiguity in Production
| Platform | % of Queries Ambiguous | How They Measure |
|----------|------------------------|------------------|
| Google | ~60% | Click entropy |
| Amazon | ~45% | Category spread |
| Netflix | ~70% | Genre spread in clicks |

---

## Types of Ambiguity (Deep Dive)

### 1. Lexical Ambiguity
Same word, multiple unrelated meanings.

| Query | Meaning 1 | Meaning 2 | Meaning 3 | Meaning 4 |
|-------|-----------|-----------|-----------|-----------|
| apple | Apple Inc | fruit | Apple Music | record label |
| python | programming | snake | Monty Python | - |
| jaguar | car | animal | macOS | Fender guitar |
| coach | brand (handbags) | sports coach | bus company | - |
| mercury | planet | element | car brand | Freddie Mercury |
| amazon | Amazon.com | river | mythology | rainforest |

**Resolution Strategy:** Entity disambiguation using context signals.

### 2. Syntactic Ambiguity
Same words, different grammatical structures.

| Query | Interpretation 1 | Interpretation 2 |
|-------|------------------|------------------|
| "small dog food" | Food for small dogs | Small bag of dog food |
| "red wine glass" | Glass for red wine | Wine glass that is red |
| "new york pizza" | Pizza from NYC | NY-style pizza anywhere |
| "wireless mouse pad" | Pad for wireless mouse | Wireless charging pad |
| "light blue running shoes" | Light blue color | Light-weight + blue |
| "cheap used cars" | Cheap + used cars | "Cheap Used Cars" (dealer name?) |

**Resolution Strategy:** Part-of-speech tagging, dependency parsing, user behavior.

### 3. Intent Ambiguity
Multiple valid actions for the same query.

| Query | Intent 1 | Intent 2 | Intent 3 |
|-------|----------|----------|----------|
| "iPhone 15" | Buy it | Compare specs | Read reviews |
| "Python" | Download it | Learn it | Get documentation |
| "Tesla" | Buy a car | Stock price | News about company |
| "Nike" | Shop Nike products | Company info | Store locator |
| "JavaScript" | Tutorials | Job listings | Libraries |

**Resolution Strategy:** Intent classification, result diversification.

### 4. Scope/Specificity Ambiguity
How narrow is the user's interest?

| Query | Narrow Interpretation | Broad Interpretation |
|-------|----------------------|---------------------|
| "laptop" | Ready to buy | Just browsing |
| "shoes" | Specific type in mind | Exploring all options |
| "restaurants" | Want to book now | Planning future trip |

**Resolution Strategy:** Session signals, click behavior, query reformulation tracking.

---

## Case Studies: How Companies Handle Ambiguity

### Case Study 1: Google "Apple" Query

**The Challenge:**
- 95% of users searching "Apple" want Apple Inc
- But 5% want the fruit
- That 5% = millions of users/day

**Google's Solution:**
```
Position 1-3: apple.com, Apple iPhone, Apple Store
Position 4: Apple - Wikipedia (company)
Position 5: Apple (fruit) - Wikipedia
Knowledge Panel: Apple Inc info
People Also Ask: 
  - "Is Apple a $3 trillion company?"
  - "What is the healthiest apple to eat?" ← Fruit intent
```

**Key Technique:** Result diversification with dominant interpretation first.

### Case Study 2: Amazon "Python" Query

**The Challenge:**
- Is user looking for: Programming books? Pet supplies? Monty Python DVDs?

**Amazon's Solution:**
1. Check user purchase history
2. Check browsing session
3. Default to most popular interpretation (books)
4. Show category tabs: "Books", "Movies", "Pet Supplies"

```
Department sidebar:
☐ Books (2,345)
☐ Movies & TV (123)
☐ Pet Supplies (45)
```

**Key Technique:** Faceted navigation for self-disambiguation.

### Case Study 3: Spotify "Sad Songs"

**The Challenge:**
- Mood-based queries are subjective
- "Sad" means different things to different people
- Classical sad vs Pop sad vs Indie sad

**Spotify's Solution:**
1. Create multiple "sad" playlists by genre
2. Personalize based on listening history
3. Show diverse results:
   - "Sad Songs" (curated playlist)
   - "Sad Indie" (genre-specific)
   - "Sad Piano" (instrument-specific)

**Key Technique:** Personalization + genre diversification.

### Case Study 4: Netflix "Action" Query

**The Challenge:**
- Action movies span many sub-genres
- User preferences vary wildly

**Netflix's Solution:**
1. Don't show generic "Action" results
2. Show personalized sub-categories:
   - "Action Thrillers" (if user likes thrillers)
   - "Action Comedies" (if user watches comedies)
   - "Classic Action" (if user has vintage preference)

**Key Technique:** Heavy personalization = implicit disambiguation.

---

## Signals for Disambiguation (Ranked by Importance)

### User-Level Signals (Strongest)
| Signal | How to Use | Lift |
|--------|------------|------|
| Purchase history | "Python" + bought programming books → programming | Very High |
| Click history | Clicked on tech articles → tech interpretation | High |
| Saved preferences | User said "I'm a developer" | Very High |
| Demographics | Age, location patterns | Medium |
| Device | IDE extension → programming | High |

### Session-Level Signals
| Signal | How to Use | Lift |
|--------|------------|------|
| Previous query | "laptop specs" → "laptop" means current laptop search | Very High |
| Referrer | Came from cooking site → food interpretation | High |
| Cart contents | iPhone in cart → Apple = tech | High |
| Time in session | Just started → exploratory | Medium |

### Query-Level Signals
| Signal | How to Use | Lift |
|--------|------------|------|
| Query length | 1 word = ambiguous, 5+ words = specific | High |
| Entities detected | "Nike" = brand, constrains interpretation | High |
| Modifiers | "how to" = informational | Very High |
| Suffix | "near me" = local | Very High |

### Population-Level Signals (Default)
| Signal | How to Use | Lift |
|--------|------------|------|
| Query popularity | "Apple" = Apple Inc (90% of clicks) | Medium |
| Time of year | "gifts" in December = holiday | Medium |
| Trending topics | After Apple event → Apple Inc | Medium |

---

## Disambiguation Techniques (Implementation)

### Technique 1: Click Entropy Measurement
```python
def calculate_click_entropy(query: str) -> float:
    """
    High entropy = high ambiguity
    Low entropy = clear intent
    """
    click_distribution = get_clicks_for_query(query)
    
    # Example: "iPhone" → {Apple: 0.95, cases: 0.03, other: 0.02}
    # Example: "apple" → {Apple Inc: 0.6, fruit: 0.25, other: 0.15}
    
    entropy = -sum(p * log2(p) for p in click_distribution.values())
    return entropy

# Interpretation:
# entropy < 1.0 → Low ambiguity (clear winner)
# entropy 1.0-2.0 → Medium ambiguity
# entropy > 2.0 → High ambiguity (diversify results)
```

### Technique 2: Result Diversification
```python
def diversify_results(results: List[Result], query: str) -> List[Result]:
    """
    Ensure multiple interpretations are covered in top results.
    """
    if is_ambiguous(query):
        interpretations = get_interpretations(query)
        # e.g., {"Apple": [...], "apple_fruit": [...]}
        
        diversified = []
        for i in range(10):
            # Round-robin through interpretations
            interp = interpretations[i % len(interpretations)]
            if interp:
                diversified.append(interp.pop(0))
        
        return diversified
    
    return results[:10]
```

### Technique 3: Query Clarification UI
```python
def should_show_clarification(query: str, entropy: float) -> bool:
    """
    Decide whether to ask user for clarification.
    """
    # High entropy + short query + no context
    if entropy > 2.0 and len(query.split()) <= 2:
        if not has_user_context():
            return True
    return False

# UI Options:
clarification_ui = {
    "type": "inline_chips",
    "options": [
        {"label": "Apple Tech", "filter": "category:electronics"},
        {"label": "Apple Fruit", "filter": "category:produce"},
        {"label": "Apple Music", "filter": "category:streaming"}
    ]
}
```

### Technique 4: Entity Linking
```python
def link_entities(query: str, context: UserContext) -> EntityLinks:
    """
    Link query terms to known entities in knowledge graph.
    """
    entities = extract_entities(query)
    
    for entity in entities:
        candidates = knowledge_graph.get_candidates(entity.text)
        
        # Score candidates based on context
        scored = []
        for candidate in candidates:
            score = 0.0
            score += context_similarity(candidate, context) * 0.5
            score += popularity(candidate) * 0.3
            score += recency(candidate) * 0.2
            scored.append((candidate, score))
        
        entity.linked = max(scored, key=lambda x: x[1])[0]
    
    return entities

# Example:
# Query: "Cook" in tech context → Tim Cook
# Query: "Cook" in food context → cooking
```

---

## When Disambiguation Fails

### Graceful Degradation
```python
def search_with_fallback(query: str, user: User) -> SearchResults:
    # Attempt 1: Personalized disambiguation
    if user.has_history():
        intent = infer_intent_from_history(query, user)
        results = search_with_intent(query, intent)
        if results.confidence > 0.8:
            return results
    
    # Attempt 2: Diversified results
    results = search_diversified(query)
    if results:
        return results
    
    # Attempt 3: Ask for clarification
    if is_highly_ambiguous(query):
        return SearchResults(
            results=search_all_interpretations(query),
            clarification_needed=True,
            clarification_options=get_clarification_options(query)
        )
    
    # Attempt 4: Best guess
    return search_most_popular_interpretation(query)
```

### Handling Wrong Guesses
```
User searched: "Apple"
System showed: Apple Inc results
User behavior: Scrolled past all, searched "apple fruit nutrition"

System learns:
1. Log reformulation: "Apple" → "apple fruit"
2. Update user model: interested in food content
3. Future "Apple" searches → higher weight on fruit interpretation
```

---

## Measuring Disambiguation Quality

### Key Metrics
| Metric | What It Measures | Target |
|--------|------------------|--------|
| Click Entropy (avg) | Overall ambiguity handling | Decreasing over time |
| Reformulation Rate | Users had to clarify | < 15% |
| Position of First Click | Was the right interpretation surfaced? | < 3 |
| Clarification Acceptance | Did users click clarification chips? | > 50% when shown |
| Diversification Coverage | Are all interpretations represented? | > 80% |

### A/B Testing Disambiguation
```python
experiment = {
    "name": "diversification_v2",
    "description": "Test increased result diversity for ambiguous queries",
    "population": "queries with entropy > 1.5",
    "control": "current_ranking",
    "treatment": "diversified_ranking",
    "metrics": [
        "click_entropy",
        "reformulation_rate", 
        "satisfaction_score",
        "conversion_rate"
    ]
}
```

---

## Key Takeaways

1. **Most queries are ambiguous** — assume ambiguity by default
2. **Context is the best disambiguator** — user history, session, location
3. **Diversify results** for high-ambiguity queries
4. **Don't force users to type more** — provide refinement UI
5. **Learn from clicks and reformulations** — data tells you what users meant
6. **Measure click entropy** — high entropy = need better disambiguation
7. **Personalization helps but isn't enough** — new users have no history
8. **Graceful degradation** — always have a fallback when disambiguation fails
