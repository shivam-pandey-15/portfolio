# Research: 2.3 Intent vs Tokens

## The Fundamental Tension
Tokens are what the user typed. Intent is what they meant. These often diverge.

---

## The Gap in Numbers

### How Big Is the Gap?
| Scenario | Token Matches | Intent Matches | Gap |
|----------|---------------|----------------|-----|
| Synonym search | 20% | 95% | 75% |
| Negation handling | 0% | 100% | 100% |
| Attribute understanding | 30% | 90% | 60% |
| Entity linking | 40% | 85% | 45% |

### Real Query Examples
| Query | Token-Based Result | Intent-Based Result |
|-------|-------------------|---------------------|
| "cheap laptop" | Laptops with "cheap" in description | Laptops under $500 |
| "not too sweet wine" | Wines with "sweet" in name | Dry wines |
| "comfortable for standing all day" | Products mentioning "standing" | Shoes with cushioning |
| "something for my sister's wedding" | Products with "wedding" tag | Formal dresses, gift ideas |

---

## Tokens: The Surface (Deep Dive)

### Tokenization Decisions in Practice

#### E-commerce Tokenization
```python
# Standard tokenizers often fail on product-specific patterns

ECOMMERCE_TOKENIZATION = {
    # SKUs should not be split
    "ABC-123-XL": ["ABC-123-XL"],  # Not ["ABC", "123", "XL"]
    
    # Sizes need special handling
    "size 10.5": ["size", "10.5"],  # Not ["size", "10", "5"]
    
    # Model numbers
    "iPhone15Pro": ["iPhone", "15", "Pro"],  # Camel case split
    
    # Price patterns
    "$49.99": ["$49.99"],  # Keep as price token
    "under $100": ["under", "$100"],
    
    # Compound product names
    "anti-aging cream": ["anti-aging", "cream"],  # Keep hyphenated
}
```

#### Code Search Tokenization
```python
# Code has unique tokenization needs

CODE_TOKENIZATION = {
    # CamelCase splitting
    "getUserById": ["get", "User", "By", "Id"],
    
    # Snake case
    "get_user_by_id": ["get", "user", "by", "id"],
    
    # Preserve operators
    "array[0]": ["array", "[", "0", "]"],
    "func()": ["func", "(", ")"],
    
    # Language-specific
    "C++": ["C++"],  # Don't split
    "node.js": ["node.js"],  # Keep together
    
    # Error messages
    "TypeError: Cannot read property": ["TypeError", "Cannot", "read", "property"]
}
```

### N-gram Strategies
| Strategy | "New York Pizza" | Index Size | Query Time |
|----------|------------------|------------|------------|
| Unigrams | ["New", "York", "Pizza"] | 1x | Fast |
| Bigrams | ["New York", "York Pizza"] | 3x | Medium |
| Trigrams | ["New York Pizza"] | 5x | Slow |
| Entity-aware | ["New York", "Pizza"] | 1.5x | Fast |

**Recommendation:** Entity-aware tokenization beats n-grams.

---

## Intent: The Depth (Deep Dive)

### Intent Understanding Layers

```
Layer 1: Lexical (Tokens)
"cheap laptop" → ["cheap", "laptop"]

Layer 2: Syntactic (Structure)
"cheap laptop" → modifier("cheap") + noun("laptop")

Layer 3: Semantic (Meaning)
"cheap laptop" → intent(purchase) + category(laptop) + constraint(price < $500)

Layer 4: Pragmatic (Context)
"cheap laptop" + user_context → intent(purchase) + category(laptop) + 
                                 constraint(price < $500) + 
                                 preference(Windows, based on history)
```

### Intent Extraction Pipeline
```python
class IntentExtractor:
    def __init__(self):
        self.ner = NERModel()
        self.attribute_parser = AttributeParser()
        self.constraint_detector = ConstraintDetector()
        
    def extract(self, query: str, context: dict) -> Intent:
        # Step 1: Entity extraction
        entities = self.ner.extract(query)
        # "Nike running shoes size 10" → {brand: Nike, category: shoes, size: 10}
        
        # Step 2: Attribute parsing
        attributes = self.attribute_parser.parse(query, entities)
        # Adds: {activity: running}
        
        # Step 3: Constraint detection
        constraints = self.constraint_detector.detect(query)
        # "under $100" → {price_max: 100}
        # "without touchscreen" → {touchscreen: False}
        
        # Step 4: Context integration
        intent = Intent(
            entities=entities,
            attributes=attributes,
            constraints=constraints,
            context_boost=self._apply_context(context)
        )
        
        return intent
```

---

## The Token-Intent Gap: Real Failures

### Case Study 1: The "Cheap" Problem (Amazon)
**Query:** "cheap laptop"
**Token search:** Matches "cheap" in title → "This Cheap Laptop Changed My Life" (review article)
**Intent search:** price < $500, category = laptop → Budget laptops sorted by price

**Root cause:** "Cheap" is an intent signal (price constraint), not a keyword to match.

**Fix:**
```python
INTENT_MODIFIERS = {
    "cheap": {"type": "price_constraint", "max_percentile": 25},
    "affordable": {"type": "price_constraint", "max_percentile": 40},
    "budget": {"type": "price_constraint", "max_percentile": 30},
    "expensive": {"type": "price_constraint", "min_percentile": 80},
    "premium": {"type": "price_constraint", "min_percentile": 90},
    "luxury": {"type": "price_constraint", "min_percentile": 95}
}
```

### Case Study 2: The Negation Problem (E-commerce)
**Query:** "laptop without touchscreen"
**Token search:** Matches "touchscreen" → Shows touchscreen laptops
**Intent search:** category = laptop, touchscreen = False → Non-touchscreen laptops

**Root cause:** Standard search engines treat all tokens as positive matches.

**Fix:**
```python
NEGATION_PATTERNS = [
    (r"without\s+(\w+)", "exclude"),
    (r"no\s+(\w+)", "exclude"),
    (r"non-?(\w+)", "exclude"),
    (r"(\w+)-free", "exclude"),
    (r"except\s+(\w+)", "exclude"),
    (r"not\s+(\w+)", "exclude")
]

def extract_negations(query: str) -> List[str]:
    exclusions = []
    for pattern, action in NEGATION_PATTERNS:
        matches = re.findall(pattern, query, re.IGNORECASE)
        if action == "exclude":
            exclusions.extend(matches)
    return exclusions

# "laptop without touchscreen" → exclusions: ["touchscreen"]
```

### Case Study 3: The Synonym Problem (Nike)
**Query:** "sneakers"
**Catalog:** Products labeled as "athletic shoes", "running shoes"
**Token search:** No "sneakers" in product titles → Zero results
**Intent search:** category includes synonyms → Shows athletic footwear

**Root cause:** Vocabulary mismatch between users and catalog.

**Fix:** Synonym table from query logs
```python
SYNONYMS = {
    "sneakers": ["athletic shoes", "running shoes", "trainers", "tennis shoes"],
    "couch": ["sofa", "loveseat", "settee", "sectional"],
    "laptop": ["notebook", "computer", "ultrabook"],
    "cell phone": ["mobile phone", "smartphone", "phone"],
}

# Build from click data
def learn_synonyms(query_click_logs):
    """If users search X and click on product Y with term Z, X and Z might be synonyms"""
    co_occurrence = defaultdict(Counter)
    for log in query_click_logs:
        query_terms = tokenize(log.query)
        product_terms = tokenize(log.clicked_product.title)
        for qt in query_terms:
            for pt in product_terms:
                if qt != pt:
                    co_occurrence[qt][pt] += 1
    
    synonyms = {}
    for term, candidates in co_occurrence.items():
        top = candidates.most_common(5)
        synonyms[term] = [c[0] for c in top if c[1] > threshold]
    return synonyms
```

### Case Study 4: The Attribute Problem (Zappos)
**Query:** "comfortable shoes for standing all day"
**Token search:** Matches "standing" in reviews → Random shoes with "standing" mentioned
**Intent search:** Understands "comfortable for standing" = need cushioning, arch support → Comfort-focused shoes

**Root cause:** User described USE CASE, not product attributes.

**Fix:** Use case to attribute mapping
```python
USE_CASE_MAPPINGS = {
    "standing all day": {
        "attributes": ["cushioned", "arch support", "wide toe box"],
        "categories": ["comfort shoes", "work shoes", "nursing shoes"],
        "boost_brands": ["Brooks", "Dansko", "New Balance"]
    },
    "running a marathon": {
        "attributes": ["lightweight", "responsive", "durable"],
        "categories": ["running shoes", "marathon shoes"],
        "boost_brands": ["Nike", "Asics", "Brooks"]
    },
    "wedding guest": {
        "attributes": ["formal", "elegant"],
        "categories": ["heels", "dress shoes", "formal wear"]
    }
}
```

---

## Bridging the Gap: Techniques

### Approach 1: Query-Time Synonym Expansion
```python
def expand_synonyms(query: str, synonym_table: dict) -> str:
    tokens = tokenize(query)
    expanded = []
    
    for token in tokens:
        if token in synonym_table:
            # Add synonyms with lower boost
            synonyms = synonym_table[token]
            expanded.append(f"({token}^2 OR {' OR '.join(synonyms)})")
        else:
            expanded.append(token)
    
    return " AND ".join(expanded)

# "sneakers under $100"
# → "(sneakers^2 OR athletic shoes OR running shoes) AND under $100"
```

### Approach 2: Semantic Search (Embeddings)
```python
def hybrid_search(query: str) -> Results:
    # Token-based retrieval (BM25)
    bm25_results = bm25_index.search(query, k=500)
    
    # Semantic retrieval (dense vectors)
    query_embedding = encoder.encode(query)
    vector_results = vector_index.search(query_embedding, k=500)
    
    # Merge with RRF (Reciprocal Rank Fusion)
    merged = reciprocal_rank_fusion([bm25_results, vector_results])
    
    return merged[:50]

# Semantic search handles:
# - "sneakers" matching "athletic shoes" (semantic similarity)
# - "comfortable for standing" matching cushioned shoes
# - "gift for dad" matching men's products
```

### Approach 3: Query Rewriting (LLM)
```python
def llm_query_rewrite(query: str) -> StructuredQuery:
    prompt = f"""
    Convert this search query into structured format:
    Query: "{query}"
    
    Output JSON with:
    - category: product category
    - attributes: list of desired attributes
    - constraints: price, size, color filters
    - exclude: things to NOT show
    """
    
    response = llm.generate(prompt)
    return json.loads(response)

# "comfortable shoes for standing all day under $100 not heels"
# → {
#     "category": "shoes",
#     "attributes": ["cushioned", "supportive", "comfort"],
#     "constraints": {"price_max": 100},
#     "exclude": ["heels", "high heels"]
# }
```

---

## Precision-Recall Trade-offs

### The Spectrum
```
Pure Token Matching ←────────────────────────→ Pure Semantic Matching
High Precision                                    High Recall
Low Recall                                        Low Precision
Many zero results                                 Many irrelevant results
```

### Optimal Operating Point by Query Type
| Query Type | Token Weight | Semantic Weight | Example |
|------------|--------------|-----------------|---------|
| Navigational | 0.9 | 0.1 | "facebook login" |
| Product SKU | 1.0 | 0.0 | "XYZ-123-ABC" |
| Category browse | 0.3 | 0.7 | "summer dresses" |
| Natural language | 0.1 | 0.9 | "gift for tech-savvy dad" |
| Attribute search | 0.5 | 0.5 | "red nike shoes size 10" |

### Dynamic Weighting
```python
def dynamic_hybrid_weight(query: str, result_count: int) -> Tuple[float, float]:
    """
    Adjust token vs semantic weight based on query and results.
    """
    # Start with balanced approach
    token_weight = 0.5
    semantic_weight = 0.5
    
    # If query is very specific (long, has entities), trust tokens more
    if len(query.split()) > 4 and has_brand(query):
        token_weight = 0.7
        semantic_weight = 0.3
    
    # If few results, lean on semantics
    if result_count < 10:
        token_weight = 0.3
        semantic_weight = 0.7
    
    # If zero results, go full semantic
    if result_count == 0:
        token_weight = 0.1
        semantic_weight = 0.9
    
    return token_weight, semantic_weight
```

---

## Key Takeaways

1. **Tokens ≠ Intent** — the literal words are just a hint
2. **Synonyms are table stakes** — must handle couch/sofa
3. **Negation is hard** — "without X" often returns X
4. **Context resolves ambiguity** — user history, session, location
5. **Hybrid approaches win** — tokens for precision, semantics for recall
6. **Learn from clicks** — real user behavior is the best synonym source
7. **Intent modifiers need special handling** — "cheap", "best", "comfortable"
8. **LLMs can help with natural language** — but add latency
