# Research: 2.5 The Query Understanding Pipeline

## Overview
Query understanding is not one step — it's a pipeline of transformations.

---

## The Pipeline

```
Raw Query: "mens nike runing shoes"
         ↓
┌────────────────────────┐
│ 1. NORMALIZATION       │  → "mens nike runing shoes" (lowercase, clean)
└────────────────────────┘
         ↓
┌────────────────────────┐
│ 2. TOKENIZATION        │  → ["mens", "nike", "runing", "shoes"]
└────────────────────────┘
         ↓
┌────────────────────────┐
│ 3. SPELL CORRECTION    │  → ["mens", "nike", "running", "shoes"]
└────────────────────────┘
         ↓
┌────────────────────────┐
│ 4. ENTITY EXTRACTION   │  → {brand: "Nike", category: "shoes", gender: "mens"}
└────────────────────────┘
         ↓
┌────────────────────────┐
│ 5. INTENT CLASSIFICATION│ → intent: "transactional"
└────────────────────────┘
         ↓
┌────────────────────────┐
│ 6. QUERY EXPANSION     │  → ["mens", "nike", "running", "shoes", "sneakers", "athletic"]
└────────────────────────┘
         ↓
┌────────────────────────┐
│ 7. QUERY REWRITING     │  → brand:Nike AND category:running_shoes AND gender:mens
└────────────────────────┘
         ↓
    Processed Query → Ready for retrieval
```

---

## Stage 1: Normalization

### What It Does
- Lowercase (or case-fold for Unicode)
- Remove extra whitespace
- Normalize Unicode (é → e, ü → u)
- Handle special characters

### Examples
| Input | Output |
|-------|--------|
| "Running SHOES" | "running shoes" |
| "café" | "cafe" |
| "   multiple   spaces  " | "multiple spaces" |
| "iPhone™" | "iphone" |

### Gotchas
- Don't normalize everything: "C++" should stay as is
- Brand capitalization may matter for exact match
- Some languages have case-specific meaning

---

## Stage 2: Tokenization

### What It Does
Splits text into tokens (words or subwords).

### Approaches
| Method | "New York City" | Trade-off |
|--------|-----------------|-----------|
| Whitespace | ["New", "York", "City"] | Misses entity |
| Punctuation | ["New", "York", "City"] | Same issue |
| N-gram | ["New", "York", "City", "New York", "York City", "New York City"] | Index bloat |
| BPE/WordPiece | ["New", "York", "City"] or merged | Subword for rare words |

### Language-Specific
- Chinese/Japanese: No spaces. Need word segmentation.
- German: Compound words. "Handschuh" = "Hand" + "Schuh" (glove)
- Arabic/Hebrew: Right-to-left, complex morphology

---

## Stage 3: Spell Correction

### What It Does
Fixes typos and misspellings.

### Approaches
| Method | Description | Example |
|--------|-------------|---------|
| Edit distance | Levenshtein distance to dictionary words | "runing" → "running" (1 edit) |
| Phonetic | Soundex, Metaphone | "niek" → "nike" (sounds like) |
| ML-based | Sequence-to-sequence models | Handles context, unseen errors |
| Query log | Common misspellings from logs | "iphoen" → "iphone" |

### Gotchas
- Brand names: "asics" looks like typo but isn't
- Product codes: "XR500" should not be corrected
- Over-correction: "pokemon" → "poker man"?

### Best Practice
- Use "Did you mean?" instead of auto-correcting
- Auto-correct only for high-confidence fixes
- Never correct if user explicitly quoted

---

## Stage 4: Entity Extraction (NER)

### What It Does
Identifies named entities in the query.

### Entity Types
| Type | Examples |
|------|----------|
| Brand | "Nike", "Apple", "Samsung" |
| Category | "shoes", "laptop", "phone" |
| Attribute | "red", "size 10", "wireless" |
| Price | "under $100", "cheap" |
| Location | "near me", "in NYC" |
| Time | "today", "this weekend" |

### Approaches
| Method | Description |
|--------|-------------|
| Dictionary | Match against known entity lists |
| Rule-based | Regex patterns: "$\d+" for price |
| CRF/HMM | Statistical sequence labeling |
| Transformer | BERT-based NER models |

### Example
```
Query: "blue nike air max size 10 under $150"
Entities:
  - color: "blue"
  - brand: "Nike"
  - product_line: "Air Max"
  - size: "10"
  - price_max: 150
```

---

## Stage 5: Intent Classification

### What It Does
Classifies what the user wants to DO.

### Intent Types
| Intent | Description | Example |
|--------|-------------|---------|
| Navigational | Go to specific page | "amazon login" |
| Informational | Learn something | "how to clean shoes" |
| Transactional | Buy/download/act | "buy iPhone 15" |
| Local | Find nearby | "pizza near me" |

### Approaches
| Method | Description |
|--------|-------------|
| Rules | "how to" → informational |
| ML Classifier | Train on labeled queries |
| Multi-label | Query can have multiple intents |

---

## Stage 6: Query Expansion

### What It Does
Adds related terms to improve recall.

### Types
| Type | Description | Example |
|------|-------------|---------|
| Synonyms | Same meaning | "couch" → "sofa" |
| Hyponyms | More specific | "fruit" → "apple", "banana" |
| Hypernyms | More general | "iPhone" → "smartphone" |
| Related | Frequently co-occurring | "camera" → "lens", "tripod" |

### Approaches
| Method | Description |
|--------|-------------|
| Manual | Curated synonym lists |
| Thesaurus | WordNet, domain-specific |
| Embedding | word2vec, BERT similarity |
| Query logs | "people who searched X also searched Y" |

### Dangers
- Over-expansion kills precision
- "jaguar" → "leopard"? (Maybe not for car searches)
- Expansion should respect entity types

---

## Stage 7: Query Rewriting

### What It Does
Transforms the query into an optimized form for the search engine.

### Examples
| Original | Rewritten |
|----------|-----------|
| "cheap laptop" | laptop AND price:[0 TO 500] |
| "Nike shoes" | brand:Nike AND category:shoes |
| "running shoes near me" | category:running_shoes AND geo_distance:[user_location, 10km] |

### Approaches
| Method | Description |
|--------|-------------|
| Rule-based | Template matching |
| Translation model | Seq2seq from query to structured |
| LLM | GPT-style query understanding |

---

## Latency Budget

### Constraint: <50ms total for query understanding

| Stage | Budget | Notes |
|-------|--------|-------|
| Normalization | 1ms | Very fast |
| Tokenization | 2ms | Language-dependent |
| Spell correction | 10ms | Can be slow, cache common |
| Entity extraction | 15ms | ML models are slow |
| Intent classification | 10ms | Simple classifier |
| Query expansion | 5ms | Lookup-based |
| Query rewriting | 5ms | Template or model |
| **Total** | **~48ms** | Tight budget! |

### Optimization Strategies
- Cache common queries (head)
- Batch ML inference
- Skip stages for simple queries
- Async for non-critical stages

---

## Key Takeaways

1. **Query understanding is a pipeline**, not a single step
2. **Each stage has trade-offs** — speed vs. accuracy
3. **Latency is critical** — total budget is ~50ms
4. **Entity extraction unlocks structured search**
5. **Expansion helps recall**, but can hurt precision
6. **Spell correction is surprisingly hard** — don't over-correct
