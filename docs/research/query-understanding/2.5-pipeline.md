# Research: 2.5 The Query Understanding Pipeline

## Overview
Query understanding is not one step — it's a pipeline of transformations that must complete in <50ms.

---

## Production Architecture

```
┌──────────────────────────────────────────────────────────────────────────┐
│                         Query Understanding Service                       │
├──────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐       │
│  │   API Gateway   │───▶│  Query Cache    │───▶│ Cache Hit?      │       │
│  │   (1ms)         │    │  (Redis)        │    │                 │       │
│  └─────────────────┘    └─────────────────┘    └────┬───────┬────┘       │
│                                                     │ Yes   │ No         │
│                                              ┌──────▼──┐    │            │
│                                              │ Return  │    │            │
│                                              │ Cached  │    │            │
│                                              └─────────┘    │            │
│                                                             ▼            │
│  ┌──────────────────────────────────────────────────────────────────┐   │
│  │                    Stage 1: Preprocessing                         │   │
│  │  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐            │   │
│  │  │ Normalization│─▶│ Tokenization │─▶│ Language     │  (5ms)     │   │
│  │  │              │  │              │  │ Detection    │            │   │
│  │  └──────────────┘  └──────────────┘  └──────────────┘            │   │
│  └──────────────────────────────────────────────────────────────────┘   │
│                                    │                                     │
│                                    ▼                                     │
│  ┌──────────────────────────────────────────────────────────────────┐   │
│  │                    Stage 2: Correction                            │   │
│  │  ┌──────────────┐  ┌──────────────┐                              │   │
│  │  │ Spell Check  │─▶│ Did You Mean │  (12ms)                      │   │
│  │  │ (SymSpell)   │  │ Generation   │                              │   │
│  │  └──────────────┘  └──────────────┘                              │   │
│  └──────────────────────────────────────────────────────────────────┘   │
│                                    │                                     │
│                                    ▼                                     │
│  ┌──────────────────────────────────────────────────────────────────┐   │
│  │                    Stage 3: Understanding (Parallel)              │   │
│  │  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐            │   │
│  │  │ NER/Entity   │  │ Intent       │  │ Attribute    │  (18ms)    │   │
│  │  │ Extraction   │  │ Classification│ │ Parsing      │            │   │
│  │  └──────────────┘  └──────────────┘  └──────────────┘            │   │
│  └──────────────────────────────────────────────────────────────────┘   │
│                                    │                                     │
│                                    ▼                                     │
│  ┌──────────────────────────────────────────────────────────────────┐   │
│  │                    Stage 4: Enhancement                           │   │
│  │  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐            │   │
│  │  │ Query        │─▶│ Synonym      │─▶│ Context      │  (10ms)    │   │
│  │  │ Expansion    │  │ Injection    │  │ Enrichment   │            │   │
│  │  └──────────────┘  └──────────────┘  └──────────────┘            │   │
│  └──────────────────────────────────────────────────────────────────┘   │
│                                    │                                     │
│                                    ▼                                     │
│  ┌──────────────────────────────────────────────────────────────────┐   │
│  │                    Stage 5: Rewriting                             │   │
│  │  ┌──────────────────────────────────────────────────────┐        │   │
│  │  │ Structured Query Generation                          │  (5ms) │   │
│  │  └──────────────────────────────────────────────────────┘        │   │
│  └──────────────────────────────────────────────────────────────────┘   │
│                                    │                                     │
│                                    ▼                                     │
│  ┌──────────────────────────────────────────────────────────────────┐   │
│  │  Output: Structured Query Object                                  │   │
│  │  Cache Result in Redis (TTL based on query frequency)             │   │
│  └──────────────────────────────────────────────────────────────────┘   │
│                                                                          │
│                         Total: ~50ms (P50: 35ms, P99: 65ms)              │
└──────────────────────────────────────────────────────────────────────────┘
```

---

## Stage 1: Preprocessing (5ms budget)

### 1.1 Normalization
```python
class QueryNormalizer:
    def normalize(self, query: str) -> str:
        # Lowercase (but preserve case for brand detection later)
        normalized = query.lower()
        
        # Unicode normalization (NFD → NFC)
        normalized = unicodedata.normalize('NFC', normalized)
        
        # Remove extra whitespace
        normalized = ' '.join(normalized.split())
        
        # Handle special characters
        normalized = self._handle_special_chars(normalized)
        
        return normalized
    
    def _handle_special_chars(self, query: str) -> str:
        # Keep: apostrophes in contractions, hyphens in compounds
        # Remove: @#$%^&* unless meaningful
        
        # Preserve programming queries
        if self._looks_like_code(query):
            return query
        
        # Standard normalization
        return re.sub(r'[^\w\s\'-]', ' ', query)
```

### 1.2 Tokenization (Domain-Specific)
```python
class DomainTokenizer:
    def __init__(self, domain: str):
        self.domain = domain
        self.compound_terms = load_compounds(domain)  # "New York", "iPhone 15"
        
    def tokenize(self, query: str) -> List[Token]:
        # First pass: compound term detection
        compounds = self._find_compounds(query)
        
        # Second pass: split remaining
        tokens = []
        position = 0
        remaining = query
        
        for compound in sorted(compounds, key=lambda x: x.start):
            # Add tokens before compound
            before = remaining[:compound.start - position]
            tokens.extend(self._simple_tokenize(before))
            
            # Add compound as single token
            tokens.append(Token(
                text=compound.text,
                type="compound",
                entities=[compound.entity_type] if compound.entity_type else []
            ))
            
            position = compound.end
            remaining = remaining[compound.end - position:]
        
        tokens.extend(self._simple_tokenize(remaining))
        return tokens

# Examples:
# "New York pizza" → ["New York", "pizza"]
# "iPhone 15 Pro case" → ["iPhone 15 Pro", "case"]
# "men's running shoes" → ["men's", "running", "shoes"]
```

### 1.3 Language Detection
```python
def detect_language(query: str) -> str:
    """
    Fast language detection for routing to right models.
    """
    # Use character-level heuristics for speed
    if any('\u4e00' <= c <= '\u9fff' for c in query):
        return 'zh'  # Chinese
    if any('\u3040' <= c <= '\u309f' for c in query):
        return 'ja'  # Japanese
    if any('\u0600' <= c <= '\u06ff' for c in query):
        return 'ar'  # Arabic
    
    # Fall back to model for ambiguous cases
    return lang_detect_model.predict(query)
```

---

## Stage 2: Spell Correction (12ms budget)

### Production Spell Correction
```python
class SpellCorrector:
    def __init__(self):
        # SymSpell for fast correction (edit distance 2)
        self.symspell = SymSpell(max_dictionary_edit_distance=2)
        self.symspell.load_dictionary("frequency_dict.txt", 0, 1)
        
        # Domain-specific terms (brands, products)
        self.protected_terms = load_protected_terms()  # "asics", "xiaomi"
        
        # Query log corrections (learned)
        self.query_corrections = load_query_corrections()
        
    def correct(self, query: str) -> CorrectionResult:
        tokens = tokenize(query)
        corrections = []
        
        for token in tokens:
            # Skip protected terms
            if token.lower() in self.protected_terms:
                continue
            
            # Check query log first (highest confidence)
            if token.lower() in self.query_corrections:
                correction = self.query_corrections[token.lower()]
                corrections.append(Correction(
                    original=token,
                    corrected=correction,
                    confidence=0.95,
                    source="query_log"
                ))
                continue
            
            # SymSpell correction
            suggestions = self.symspell.lookup(
                token, 
                Verbosity.CLOSEST,
                max_edit_distance=2
            )
            
            if suggestions and suggestions[0].distance < 2:
                corrections.append(Correction(
                    original=token,
                    corrected=suggestions[0].term,
                    confidence=self._confidence_score(suggestions[0]),
                    source="symspell"
                ))
        
        return CorrectionResult(
            original=query,
            corrected=apply_corrections(query, corrections),
            corrections=corrections,
            should_autocorrect=all(c.confidence > 0.9 for c in corrections),
            did_you_mean=corrections if not all(c.confidence > 0.9 for c in corrections) else None
        )
```

### Common Mistakes to Avoid
| Mistake | Example | Solution |
|---------|---------|----------|
| Over-correcting brands | "asics" → "basics" | Protect known brands |
| Correcting product codes | "XR500" → "XR 500" | Detect alphanumeric patterns |
| Breaking compound terms | "macbook" → "mac book" | Compound term dictionary |
| Ignoring context | "python" → "pithon" | Context-aware models |

---

## Stage 3: Understanding (18ms budget, parallelized)

### Run in Parallel
```python
async def understand_query(query: str, context: UserContext) -> QueryUnderstanding:
    # Run NER, intent, and attribute extraction in parallel
    ner_task = asyncio.create_task(extract_entities(query))
    intent_task = asyncio.create_task(classify_intent(query, context))
    attribute_task = asyncio.create_task(parse_attributes(query))
    
    # Wait for all (parallel execution)
    entities, intent, attributes = await asyncio.gather(
        ner_task, intent_task, attribute_task
    )
    
    return QueryUnderstanding(
        entities=entities,
        intent=intent,
        attributes=attributes
    )
```

### 3.1 Entity Extraction (NER)
```python
class EntityExtractor:
    def __init__(self):
        # Pre-trained NER (distilled BERT, ~10ms inference)
        self.model = load_ner_model("ner_distilled_v3")
        
        # Domain dictionaries for post-processing
        self.brand_dict = load_brands()
        self.category_dict = load_categories()
        
    def extract(self, query: str) -> List[Entity]:
        # Model inference
        model_entities = self.model.predict(query)
        
        # Dictionary matching (for precision)
        dict_entities = self._dictionary_match(query)
        
        # Merge (prefer dictionary for known entities)
        merged = self._merge_entities(model_entities, dict_entities)
        
        return merged

# Result for "Nike Air Max 90 size 10 under $100":
# [
#   Entity(text="Nike", type="BRAND", confidence=0.99),
#   Entity(text="Air Max 90", type="PRODUCT_LINE", confidence=0.95),
#   Entity(text="size 10", type="SIZE", confidence=0.98),
#   Entity(text="under $100", type="PRICE_CONSTRAINT", confidence=0.97)
# ]
```

### 3.2 Intent Classification
```python
class IntentClassifier:
    def __init__(self):
        # Fast classifier (XGBoost or small neural net)
        self.model = load_intent_model("intent_xgb_v2")
        
    def classify(self, query: str, context: UserContext) -> IntentResult:
        features = self._extract_features(query, context)
        
        probabilities = self.model.predict_proba(features)
        
        return IntentResult(
            navigational=probabilities["navigational"],
            informational=probabilities["informational"],
            transactional=probabilities["transactional"],
            local=probabilities["local"],
            primary=max(probabilities, key=probabilities.get)
        )
    
    def _extract_features(self, query: str, context: UserContext) -> dict:
        return {
            # Query features
            "query_length": len(query.split()),
            "has_question_word": any(q in query.lower() for q in ["how", "what", "why", "when"]),
            "has_transaction_word": any(t in query.lower() for t in ["buy", "order", "price"]),
            "has_location": "near me" in query.lower() or has_geo_entity(query),
            
            # Context features
            "is_mobile": context.device == "mobile",
            "is_logged_in": context.user_id is not None,
            "hour_of_day": context.hour,
            "has_previous_query": context.previous_query is not None
        }
```

---

## Stage 4: Enhancement (10ms budget)

### 4.1 Query Expansion
```python
class QueryExpander:
    def __init__(self):
        self.synonym_table = load_synonyms()
        self.embedding_model = load_embedding_model("e5-small")  # Fast model
        
    def expand(self, query: str, entities: List[Entity]) -> ExpandedQuery:
        expansions = []
        
        for token in tokenize(query):
            # Don't expand entities (brands, specific attributes)
            if any(e.text.lower() == token.lower() for e in entities):
                continue
            
            # Synonym expansion
            if token in self.synonym_table:
                synonyms = self.synonym_table[token][:3]  # Limit expansions
                expansions.extend([
                    Expansion(original=token, expanded=syn, weight=0.8)
                    for syn in synonyms
                ])
        
        return ExpandedQuery(
            original=query,
            expansions=expansions,
            expanded_query=self._build_expanded_query(query, expansions)
        )
```

### 4.2 Context Enrichment
```python
def enrich_with_context(query: QueryUnderstanding, context: UserContext) -> QueryUnderstanding:
    """
    Add implicit signals from user context.
    """
    enriched = query.copy()
    
    # Add personalization signals
    if context.user_id:
        user_profile = get_user_profile(context.user_id)
        enriched.personalization = {
            "preferred_brands": user_profile.top_brands[:3],
            "price_sensitivity": user_profile.price_percentile,
            "size_preferences": user_profile.sizes
        }
    
    # Add session signals
    if context.previous_query:
        enriched.session = {
            "continuation": is_continuation(context.previous_query, query.raw),
            "refinement": is_refinement(context.previous_query, query.raw)
        }
    
    # Add temporal signals
    enriched.temporal = {
        "is_weekend": context.day_of_week in [5, 6],
        "is_evening": context.hour >= 18,
        "season": get_season(context.date)
    }
    
    return enriched
```

---

## Stage 5: Query Rewriting (5ms budget)

### Generate Structured Query
```python
class QueryRewriter:
    def rewrite(self, understanding: QueryUnderstanding) -> StructuredQuery:
        # Build filters from entities
        filters = {}
        for entity in understanding.entities:
            if entity.type == "BRAND":
                filters["brand"] = entity.text
            elif entity.type == "SIZE":
                filters["size"] = self._parse_size(entity.text)
            elif entity.type == "PRICE_CONSTRAINT":
                price_filter = self._parse_price(entity.text)
                filters.update(price_filter)
            elif entity.type == "COLOR":
                filters["color"] = entity.text
        
        # Build text query (with expansions)
        text_parts = []
        for token in understanding.tokens:
            if not any(e.text.lower() == token.lower() for e in understanding.entities):
                text_parts.append(token)
        
        # Add synonym expansions
        for expansion in understanding.expansions:
            text_parts.append(f"({expansion.original} OR {expansion.expanded})")
        
        return StructuredQuery(
            text_query=" ".join(text_parts),
            filters=filters,
            intent=understanding.intent.primary,
            boosts=self._compute_boosts(understanding),
            personalization=understanding.personalization
        )
```

### Output Example
```python
# Input: "mens nike running shoes size 10 under $100"
# 
# Output:
StructuredQuery(
    text_query="(running OR jogging) shoes (sneakers OR athletic)",
    filters={
        "brand": "Nike",
        "department": "Mens",
        "size": "10",
        "price": {"max": 100}
    },
    intent="transactional",
    boosts={
        "in_stock": 1.5,
        "prime_eligible": 1.2,  # If user is Prime member
        "high_rating": 1.1
    },
    personalization={
        "preferred_brands": ["Nike", "Adidas"],
        "size_preferences": {"shoes": "10"}
    }
)
```

---

## Latency Optimization Techniques

### 1. Caching Strategy
```python
CACHE_CONFIG = {
    "head_queries": {  # Top 10K queries
        "ttl": 3600,  # 1 hour
        "warm": True  # Pre-warm cache
    },
    "torso_queries": {  # Next 100K
        "ttl": 600,  # 10 minutes
        "warm": False
    },
    "tail_queries": {
        "ttl": 60,  # 1 minute
        "warm": False
    }
}

def get_cached_or_process(query: str) -> QueryUnderstanding:
    cache_key = f"qu:{hash(query)}"
    
    cached = redis.get(cache_key)
    if cached:
        return deserialize(cached)
    
    result = process_query(query)
    
    ttl = get_ttl_for_query(query)
    redis.set(cache_key, serialize(result), ex=ttl)
    
    return result
```

### 2. Model Optimization
| Technique | Latency Reduction | Accuracy Trade-off |
|-----------|-------------------|-------------------|
| Model distillation | 3x faster | -2% accuracy |
| Quantization (INT8) | 2x faster | -1% accuracy |
| ONNX Runtime | 1.5x faster | 0% |
| Batching | 2x throughput | +5ms latency |
| GPU (if available) | 5x faster | 0% |

### 3. Parallel Execution
```python
# Instead of sequential:
# normalize → tokenize → spell_check → ner → intent → expand → rewrite
# 
# Run in parallel where possible:
# 
# normalize → tokenize ─┬─ spell_check
#                       ├─ ner ────────────┬─ expand → rewrite
#                       └─ intent ─────────┘
```

---

## Monitoring Dashboard

### Key Metrics
| Metric | Target | Alert Threshold |
|--------|--------|-----------------|
| P50 Latency | 35ms | >50ms |
| P99 Latency | 65ms | >100ms |
| Cache Hit Rate | 70% | <50% |
| Spell Correction Rate | 5-10% | >20% (bad data?) |
| NER F1 Score | >0.92 | <0.85 |
| Intent Accuracy | >0.88 | <0.80 |

### Latency Breakdown Dashboard
```
┌────────────────────────────────────────────────────────────┐
│  Query Understanding Latency (P50)                         │
├────────────────────────────────────────────────────────────┤
│  Preprocessing ███░░░░░░░░░░░░░░░░░  5ms (14%)             │
│  Spell Check   ████████░░░░░░░░░░░░  12ms (34%)            │
│  NER           ██████████░░░░░░░░░░  10ms (29%)            │
│  Intent        ███░░░░░░░░░░░░░░░░░  3ms (8%)              │
│  Expansion     ██░░░░░░░░░░░░░░░░░░  3ms (8%)              │
│  Rewriting     ██░░░░░░░░░░░░░░░░░░  2ms (6%)              │
│  ─────────────────────────────────────────────────────     │
│  Total         ███████████████████░  35ms                  │
└────────────────────────────────────────────────────────────┘
```

---

## Key Takeaways

1. **Query understanding is a pipeline**, not a single step
2. **Each stage has trade-offs** — speed vs. accuracy
3. **Latency is critical** — total budget is ~50ms
4. **Entity extraction unlocks structured search**
5. **Expansion helps recall**, but can hurt precision
6. **Spell correction is surprisingly hard** — don't over-correct
7. **Parallelize where possible** — NER and intent can run together
8. **Cache aggressively** — head queries should hit cache
