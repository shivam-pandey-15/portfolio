# Research: 2.1 What a Query Really Is

## Beyond the Text Box
A query is not just text. It's a **compressed expression of user intent** with massive information loss.

---

## The Anatomy of a Query

### What the User Typed
```
running shoes
```

### What the User Actually Meant
```
{
  "intent": "purchase",
  "category": "athletic footwear",
  "activity": "running (jogging, not sprinting)",
  "gender": "inferred from user profile or unspecified",
  "size": "unknown (will filter later)",
  "price_range": "mid-range (assumed)",
  "brand_preference": "none specified",
  "urgency": "unknown",
  "context": "probably replacing old shoes"
}
```

### Information Lost in Compression
| Dimension | User Knows | System Knows |
|-----------|------------|--------------|
| Activity type | Running, not walking | Maybe (if good taxonomy) |
| Size | Their foot size | Only if logged in |
| Price sensitivity | Their budget | Only from past purchases |
| Brand preference | Their favorites | Only from history |
| Urgency | Need it tomorrow | Unknown |
| Foot condition | Wide feet, flat arches | Never (unless explicit) |

---

## Industry Deep Dive: How Major Companies Parse Queries

### Amazon's Query Understanding
**Input:** "mens nike running shoes size 10"

**Parsed Structure:**
```json
{
  "tokens": ["mens", "nike", "running", "shoes", "size", "10"],
  "entities": {
    "gender": "mens",
    "brand": "Nike",
    "product_type": "running_shoes",
    "size": "10"
  },
  "intent": "product_search",
  "category_id": "shoes_men_athletic_running",
  "filters": {
    "brand": ["Nike"],
    "size": ["10"],
    "department": ["Men"]
  }
}
```

**What Amazon adds (implicit):**
- Prime eligibility (based on user membership)
- Price personalization (based on purchase history)
- Availability at nearest warehouse
- Previous brand affinities

### Google's Query Understanding
**Input:** "best pizza near me"

**Parsed Structure:**
```json
{
  "query_type": "local_search",
  "intent": "find_business",
  "qualifiers": ["best"],
  "entity_type": "restaurant",
  "cuisine": "pizza",
  "location": {
    "type": "near_user",
    "coordinates": [40.7128, -74.0060]
  },
  "implicit_filters": {
    "open_now": true,
    "rating_min": 4.0
  }
}
```

### Spotify's Query Understanding
**Input:** "sad songs for rainy days"

**Parsed Structure:**
```json
{
  "query_type": "mood_search",
  "mood": "sad",
  "context": "rainy_weather",
  "intent": "playlist_discovery",
  "expansion": ["melancholy", "acoustic", "slow tempo"],
  "personalization": {
    "preferred_genres": ["indie", "folk"],
    "recent_artists": ["Bon Iver", "Phoebe Bridgers"]
  }
}
```

---

## Query Components

### 1. Tokens (Words)
The raw text, split by whitespace or punctuation.
- "running shoes" → ["running", "shoes"]
- "men's nike air max" → ["men's", "nike", "air", "max"]

**Tokenization Edge Cases:**
| Input | Naive | Smart |
|-------|-------|-------|
| "C++" | ["C"] | ["C++"] |
| "iPhone15Pro" | ["iPhone15Pro"] | ["iPhone", "15", "Pro"] |
| "state-of-the-art" | ["state", "of", "the", "art"] | ["state-of-the-art"] |
| "New York" | ["New", "York"] | ["New York"] (entity) |

### 2. Entities
Named entities extracted from the query.

**Entity Types by Domain:**

| Domain | Entity Types | Examples |
|--------|--------------|----------|
| E-commerce | Brand, Category, Attribute, Price | Nike, shoes, red, under $100 |
| Travel | Destination, Date, Class | Paris, next Friday, business class |
| Food | Cuisine, Diet, Ingredient | Italian, vegan, gluten-free |
| Media | Genre, Artist, Mood | comedy, Taylor Swift, upbeat |
| Enterprise | Person, Date, Project | John Smith, Q3, Project Alpha |

### 3. Intent
What the user wants to DO with the results.

**Intent Taxonomy (Expanded):**
| Intent | Sub-types | Examples |
|--------|-----------|----------|
| Navigational | Site, Page, Login | "facebook", "gmail login" |
| Informational | How-to, Definition, Comparison | "how to tie a tie", "iPhone vs Samsung" |
| Transactional | Buy, Download, Subscribe | "buy iPhone 15", "download Spotify" |
| Local | Find, Directions, Hours | "pizza near me", "target hours" |
| Commercial Investigation | Reviews, Best, Top | "best laptop 2024", "Tesla reviews" |

### 4. Context (Implicit)
Information not in the query but affects results.

**Context Signals by Importance:**
| Signal | Weight | Availability | Example |
|--------|--------|--------------|---------|
| Previous query (same session) | Very High | Always | User searched "iPhone 15", now searches "case" |
| User purchase history | High | Logged in | Previously bought Nike → boost Nike |
| Location | High | With permission | "pizza" → local results |
| Device type | Medium | Always | Mobile → prioritize quick actions |
| Time of day | Medium | Always | 8am "coffee" → cafes, not beans |
| Day of week | Low | Always | Friday "dinner" → restaurants |

---

## Real-World Case Studies

### Case Study 1: Flipkart's Big Billion Days Query Surge

**Challenge:** 100M+ searches in 4 hours during sale event.

**Query patterns observed:**
| Query Type | % of Traffic | Example |
|------------|--------------|---------|
| Category + Price | 35% | "mobile under 15000" |
| Brand + Product | 25% | "Samsung S24" |
| Offer queries | 20% | "laptop discount", "sale on TV" |
| Comparison | 10% | "iPhone vs Samsung" |
| Navigational | 10% | "my orders", "track order" |

**Key insight:** During sales, price becomes the dominant signal. "under X" queries increased 5x.

### Case Study 2: Stack Overflow's Search Evolution

**Problem:** Developers search differently than consumers.

**Query characteristics:**
- High use of symbols: "array[0]", "function()", "==="
- Code snippets as queries: "for i in range(10)"
- Error messages: "TypeError: Cannot read property 'x' of undefined"
- Exact match critical: "useEffect" ≠ "use effect"

**Solution:** 
- Custom tokenizer that preserves code syntax
- Error message fingerprinting
- Tag-aware ranking (Python questions rank higher in Python tag)

### Case Study 3: Netflix's "I don't know what I want" Problem

**Challenge:** 60% of users don't search. They browse.

**Query patterns for those who do search:**
| Pattern | % of Searches | Challenge |
|---------|---------------|-----------|
| Title search | 40% | Easy — exact match |
| Actor search | 20% | Entity linking required |
| Genre search | 15% | Subjective ("funny") |
| Mood search | 10% | Requires content understanding |
| Quote search | 5% | "I am Iron Man" → find movie |
| Vague | 10% | "that show" — needs context |

**Solution:** Heavy personalization. Same query, different results per user.

---

## Query Richness Spectrum

### Sparse Queries (Hard) — The 1-2 Word Problem

**Distribution:** ~50% of all queries are 1-2 words

| Query | Possible Intents | Difficulty |
|-------|------------------|------------|
| "apple" | Company, fruit, music, store | Very High |
| "shoes" | Men's, women's, athletic, formal | Very High |
| "python" | Language, snake, Monty Python | Very High |

**Strategies:**
1. Default to most popular intent (based on click data)
2. Show diversified results
3. Provide facets for disambiguation
4. Use user context heavily

### Rich Queries (Easier) — The Specific Intent

| Query | Parsed Intent |
|-------|---------------|
| "men's nike air max 90 size 11 black" | Exact product match |
| "cheap hotels in Paris December 15-20" | Constrained search |
| "how to debug Python segmentation fault macOS" | Specific problem |

**Strategies:**
1. Parse all constraints
2. Apply filters before ranking
3. If no exact match, relax constraints gracefully

### Natural Language Queries (Hardest) — The AI Challenge

| Query | Required Understanding |
|-------|------------------------|
| "I need something comfortable for my marathon next month" | Activity → marathon, Attribute → comfortable, Timeframe → next month |
| "laptop for my kid's homework that won't break the bank" | User → child, Use case → homework, Price → budget |
| "that song that goes da da da" | Audio matching, melody recognition |

**Modern Approaches:**
- LLM-based query understanding
- Multimodal matching (audio, image)
- Conversational clarification

---

## The Query Understanding Pipeline (Detailed)

```
Raw Query: "runing shoez nike under $100"
         ↓
┌────────────────────────────────────────────────────────────────┐
│ 1. NORMALIZATION (1ms)                                         │
│    - Lowercase: "runing shoez nike under $100"                 │
│    - Unicode normalize: (no change needed)                     │
│    - Trim whitespace: (no change needed)                       │
└────────────────────────────────────────────────────────────────┘
         ↓
┌────────────────────────────────────────────────────────────────┐
│ 2. TOKENIZATION (2ms)                                          │
│    - Split: ["runing", "shoez", "nike", "under", "$100"]       │
│    - Preserve: "$100" as price token                           │
└────────────────────────────────────────────────────────────────┘
         ↓
┌────────────────────────────────────────────────────────────────┐
│ 3. SPELL CORRECTION (10ms)                                     │
│    - "runing" → "running" (confidence: 0.95)                   │
│    - "shoez" → "shoes" (confidence: 0.92)                      │
│    - "nike" → "Nike" (brand capitalization)                    │
│    - Result: ["running", "shoes", "Nike", "under", "$100"]     │
└────────────────────────────────────────────────────────────────┘
         ↓
┌────────────────────────────────────────────────────────────────┐
│ 4. ENTITY EXTRACTION (15ms)                                    │
│    - Brand: Nike (confidence: 0.99)                            │
│    - Category: running shoes (confidence: 0.95)                │
│    - Price constraint: max=$100 (confidence: 0.98)             │
└────────────────────────────────────────────────────────────────┘
         ↓
┌────────────────────────────────────────────────────────────────┐
│ 5. INTENT CLASSIFICATION (5ms)                                 │
│    - Intent: transactional (confidence: 0.88)                  │
│    - Sub-intent: product_search                                │
└────────────────────────────────────────────────────────────────┘
         ↓
┌────────────────────────────────────────────────────────────────┐
│ 6. QUERY EXPANSION (5ms)                                       │
│    - Synonyms: running → athletic, jogging                     │
│    - Related: shoes → sneakers, footwear                       │
│    - NOT expanded: Nike (brand), $100 (constraint)             │
└────────────────────────────────────────────────────────────────┘
         ↓
┌────────────────────────────────────────────────────────────────┐
│ 7. QUERY REWRITING (5ms)                                       │
│    Final Query:                                                │
│    {                                                           │
│      "text_match": "running shoes sneakers athletic",          │
│      "filters": {                                              │
│        "brand": ["Nike"],                                      │
│        "price": {"max": 100}                                   │
│      },                                                        │
│      "category": "athletic_footwear"                           │
│    }                                                           │
└────────────────────────────────────────────────────────────────┘
         ↓
    Total: 43ms → Ready for retrieval
```

---

## Failure Case Studies

### Failure 1: The Negation Problem
**Query:** "laptop without touchscreen"
**Bad result:** Laptops WITH touchscreen
**Why:** System matched "laptop" and "touchscreen" as keywords

**Fix:** Negation detection in entity extraction
```
"without X" → filter: NOT X
"no X" → filter: NOT X
"except X" → filter: NOT X
```

### Failure 2: The Over-Correction Problem
**Query:** "asics running shoes"
**Bad result:** "basics running shoes" (corrected brand name)
**Why:** Spell checker thought "asics" was typo for "basics"

**Fix:** 
- Whitelist known brands
- Higher threshold for brand corrections
- "Did you mean?" instead of auto-correct

### Failure 3: The Context Blindness Problem
**Query:** "jaguar" (user was browsing car websites)
**Bad result:** Jaguar (animal) - Wikipedia article
**Why:** System ignored browsing context

**Fix:**
- Session-based context weighting
- Domain detection from referrer
- Recent query influence

---

## Technical Implementation: Query Understanding Service

### Architecture Pattern
```
┌─────────────────────────────────────────────────────────────┐
│                    API Gateway                               │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│               Query Understanding Service                    │
│  ┌──────────────────────────────────────────────────────┐   │
│  │ Query Cache (Redis) - Check if we've seen this query │   │
│  └──────────────────────────────────────────────────────┘   │
│                            ↓                                 │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐       │
│  │ Spell Check  │  │    NER       │  │   Intent     │       │
│  │   (CPU)      │  │  (GPU/TRT)   │  │   (CPU)      │       │
│  └──────────────┘  └──────────────┘  └──────────────┘       │
│                            ↓                                 │
│  ┌──────────────────────────────────────────────────────┐   │
│  │           Query Expansion (Embedding Service)         │   │
│  └──────────────────────────────────────────────────────┘   │
│                            ↓                                 │
│  ┌──────────────────────────────────────────────────────┐   │
│  │           Query Rewriter (Rules + ML)                 │   │
│  └──────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
                              ↓
              Structured Query → Retrieval Service
```

### Latency Budget Breakdown
| Component | Budget | P50 | P99 |
|-----------|--------|-----|-----|
| Cache lookup | 5ms | 1ms | 3ms |
| Tokenization | 2ms | 0.5ms | 1ms |
| Spell correction | 10ms | 5ms | 15ms |
| NER | 15ms | 8ms | 20ms |
| Intent | 5ms | 2ms | 8ms |
| Expansion | 5ms | 2ms | 5ms |
| Rewriting | 5ms | 2ms | 5ms |
| **Total** | **47ms** | **20ms** | **57ms** |

---

## Key Takeaways

1. **A query is compressed intent**, not just text
2. **Context is as important as content** (who, where, when)
3. **Most queries are ambiguous** — the system must handle this
4. **Rich queries are easier than sparse queries** — but rarer
5. **The goal is intent satisfaction**, not keyword matching
6. **Every major company has custom query understanding** — no off-the-shelf solution works perfectly
7. **Latency is critical** — users won't wait
8. **Spell correction is surprisingly hard** — beware of over-correction
