# Chapter 5.4: Filters, Facets, and Constraints

## Overview

This chapter covers the mechanisms that refine search results beyond full-text matching. While retrieval (5.2, 5.3) finds *candidates*, Filters/Facets/Constraints determine *which* candidates are valid and provide navigation affordances to users.

---

## 1. Definitions & Distinctions

| Concept | What It Does | Affects Score? | Cacheable? |
|---------|--------------|----------------|------------|
| **Filter** | Binary exclusion (yes/no) | ❌ No | ✅ Highly |
| **Facet** | Counts values across result set | N/A | Partial |
| **Constraint** | The parameter applied (e.g., `price < 100`) | Depends | Via Filter |

### Key Insight
- **Filters** answer: "Is this document in or out?"
- **Facets** answer: "How many documents have each value?"
- **Constraints** are the *specification* that drives both.

---

## 2. Core Data Structures

### 2.1 Inverted Index for Filtering

```
Term "Blue" → [DocID: 1, 5, 12, 99, 103, ...]
Term "Red"  → [DocID: 2, 5, 14, 99, ...]
```

Filtering = set intersection. Query `Blue AND Red` → intersect posting lists → `[5, 99]`

**Representation Options:**
- **Sorted Arrays:** Space-efficient, binary search for membership
- **Bitsets:** O(1) membership check, O(1) AND/OR via CPU bitwise ops
- **Roaring Bitmaps:** Hybrid compression for real-world distributions

### 2.2 Roaring Bitmaps (Industry Standard)

Roaring divides the 32-bit docID space into 65,536 "containers" (each covering 65,536 docIDs).

| Container Type | Used When | Storage |
|----------------|-----------|---------|
| **Array Container** | ≤4096 values | Sorted u16 array |
| **Bitmap Container** | >4096 values | 8KB bitmap |
| **Run Container** | Consecutive runs | Run-length encoded |

**Why Roaring Wins:**
- Adapts to data density automatically
- Bitwise AND/OR operations work across container types
- 10-100x faster than naive bitsets for sparse data

### 2.3 BKD Trees for Numeric/Geo Constraints

Since Lucene 6.0, numeric range queries use **BKD trees** (Block K-Dimensional trees).

**Structure:**
- Multi-dimensional point index (supports 1D to N-dimensional)
- Balanced tree of blocks (leaves contain ~1024 points)
- Stored as sorted byte arrays (lexicographically comparable)

**Query Execution:**
1. Traverse tree, checking if node's bounding box intersects query range
2. **Entirely inside range:** Accept all points in subtree (fast)
3. **Entirely outside range:** Skip subtree (fast)
4. **Crossing boundary:** Recurse deeper (slower)

**Performance Tips:**
- Index-sort documents by the most-queried numeric field
- High-precision ranges crossing many leaf boundaries are slower
- Use Point fields only if you need range queries; use DocValues if only for sorting/aggregation

### 2.4 Doc Values (Column Store) for Faceting

**The Problem:**
Inverted index answers: "Which docs have term X?"
Faceting needs: "What terms does doc X have?"

**Solution: Doc Values**
- Column-oriented storage (one file per field)
- Maps DocID → Value(s)
- Stored on disk, read via OS filesystem cache (not JVM heap)

**Compression Techniques:**
- Delta encoding for sorted numeric values
- GCD-based compression when values share common divisor
- Bit-packing for small integer ranges
- Dictionary encoding for low-cardinality strings

**Memory Model:**
```
                    JVM Heap (GC-managed)
                           ↑
                    NOT used by DocValues
                           
   Disk (.dvd files) ←→ OS Filesystem Cache ←→ CPU
                         (mmap'd)
```

---

## 3. Filter vs Query: The Fundamental Split

### In Elasticsearch/OpenSearch Terms

```json
{
  "query": {
    "bool": {
      "must": [
        { "match": { "title": "laptop" } }    // Affects score
      ],
      "filter": [
        { "term": { "in_stock": true } },     // No score, cached
        { "range": { "price": { "lte": 1000 }}} // No score, cached
      ]
    }
  }
}
```

**Execution Order:**
1. Filters run first (fast, cached)
2. Scoring clauses run on filtered subset
3. Result: Smaller candidate set for expensive scoring

### Filter Caching

Filters produce **bitsets** that can be cached and reused.

```
Cache Key: hash(field="in_stock", value=true, segment_id=abc123)
Cache Value: RoaringBitmap [1, 4, 5, 7, 12, ...]
```

**Cache Behavior:**
- Per-segment (each segment has its own filter cache)
- LRU eviction when memory pressure
- Automatic invalidation on segment merge

---

## 4. Faceted Navigation Architecture

### 4.1 The UI Pattern

User sees:
```
Category: Electronics (1,234) | Clothing (567) | Books (890)
Brand:    Apple (100) | Samsung (80) | Sony (45) [+10 more]
Price:    $0-50 (200) | $50-100 (350) | $100+ (884)
```

### 4.2 Computing Facet Counts

**Algorithm:**
1. Execute base query → get result bitset
2. For each facet field:
   a. Iterate over bits set to 1 in result
   b. For each docID, lookup value in DocValues
   c. Increment counter for that value
3. Return top-N values by count

**Pseudocode:**
```python
def compute_facet(result_bitset: RoaringBitmap, 
                  field: str, 
                  doc_values: DocValues) -> dict[str, int]:
    counts = defaultdict(int)
    for doc_id in result_bitset:
        value = doc_values.get(doc_id, field)
        counts[value] += 1
    return dict(sorted(counts.items(), key=lambda x: -x[1]))
```

### 4.3 The Post-Filter Pattern

**Problem:** User selects "Color: Red". You want:
- Results: Only red products
- Color facet: Still shows Blue (40), Green (20) so user can switch

**Solution: post_filter**

```json
{
  "query": { "match": { "title": "shoes" }},
  "post_filter": { "term": { "color": "red" }},
  "aggs": {
    "colors": { "terms": { "field": "color" }}
  }
}
```

**Execution:**
1. Query runs → gets shoes
2. Aggregations run → counts all colors in shoes
3. Post-filter runs → filters hits to only red
4. Return: filtered hits + unfiltered facets

### 4.4 Cross-Filter Pattern (Full Faceted Navigation)

**Problem:** User selects "Red" AND "Size 10".
- Color facet should show colors available in size 10 (but not filter by color)
- Size facet should show sizes available in red (but not filter by size)

**Solution: Per-facet filter aggregations**

```json
{
  "query": { "match": { "category": "shoes" }},
  "post_filter": {
    "bool": {
      "filter": [
        { "term": { "color": "red" }},
        { "term": { "size": "10" }}
      ]
    }
  },
  "aggs": {
    "color_facet": {
      "filter": { "term": { "size": "10" }},  // Apply size, NOT color
      "aggs": {
        "colors": { "terms": { "field": "color" }}
      }
    },
    "size_facet": {
      "filter": { "term": { "color": "red" }},  // Apply color, NOT size
      "aggs": {
        "sizes": { "terms": { "field": "size" }}
      }
    }
  }
}
```

---

## 5. Distributed Faceting

### The Scatter-Gather Problem

**Setup:** Index has 5 shards. Query for "laptop" with facet on "brand".

**Naive Approach:**
1. Each shard returns its top-10 brands
2. Coordinator merges

**Problem:** Shard 1 might have `Lenovo: 50`, but it's #11 so not returned. 
Global top-10 becomes inaccurate.

**Solutions:**

| Approach | Trade-off |
|----------|-----------|
| **Return more per shard** | `shard_size = size * 1.5 + 10` (Elasticsearch default) |
| **Show doc_count_error_upper_bound** | Transparency about inaccuracy |
| **Use `composite` aggregation** | Pagination through all values accurately |

### Cardinality Estimation

For high-cardinality fields (usernames, product IDs), exact counting is expensive.

**HyperLogLog++ (HLL++):**
- Probabilistic cardinality estimator
- Fixed memory (~40KB for 2-3% error)
- Elasticsearch's `cardinality` aggregation

---

## 6. Performance Optimization

### 6.1 Index-Time

| Strategy | Impact |
|----------|--------|
| **Use `keyword` not `text` for facets** | Enables DocValues, faster aggregation |
| **Enable DocValues explicitly** | Default for non-`text` fields |
| **Index sorting** | Physical contiguity → better cache locality |
| **Avoid high-cardinality facets** | Consider bucketing or approximation |

### 6.2 Query-Time

| Strategy | Impact |
|----------|--------|
| **Put selective filters first** | Reduces candidate set early |
| **Use filter cache** | Reuse bitsets across queries |
| **Limit facet size** | `"size": 10` not `"size": 10000` |
| **Use `include`/`exclude`** | Regex or exact list to reduce facet terms |
| **Paginate with `composite`** | For exhaustive faceting |

### 6.3 Memory

```
Filesystem Cache Priority:
1. .dvd (DocValues data) - Faceting
2. .dim/.dii (BKD tree) - Numeric filters
3. .tip/.tim (Term index) - Text filters
4. Postings files - Scoring
```

**Rule of Thumb:** Leave 50% of RAM for OS filesystem cache.

---

## 7. Hierarchical Facets

### Example: Category Taxonomy

```
Electronics
├── Phones
│   ├── Smartphones
│   └── Feature Phones
└── Laptops
    ├── Gaming
    └── Business
```

### Implementation Approaches

**1. Path Tokenization:**
```
doc.category_path = ["Electronics", "Electronics/Phones", "Electronics/Phones/Smartphones"]
```
Facet on all paths, display hierarchically in UI.

**2. Separate Fields:**
```
doc.category_l1 = "Electronics"
doc.category_l2 = "Phones"
doc.category_l3 = "Smartphones"
```
Aggregate on each level independently.

**3. Nested/Parent-Child:**
Complex but allows independent updates to taxonomy.

---

## 8. Range Facets

### Defined Ranges (Buckets)

```json
{
  "aggs": {
    "price_ranges": {
      "range": {
        "field": "price",
        "ranges": [
          { "to": 50 },
          { "from": 50, "to": 100 },
          { "from": 100, "to": 200 },
          { "from": 200 }
        ]
      }
    }
  }
}
```

### Auto-Generated Ranges (Histogram)

```json
{
  "aggs": {
    "price_histogram": {
      "histogram": {
        "field": "price",
        "interval": 50
      }
    }
  }
}
```

---

## 9. Key Takeaways

1. **Filters don't score** - they're binary, cacheable, and run first
2. **Faceting needs DocValues** - column-oriented storage for efficiency
3. **BKD trees for numerics** - O(log n) range queries
4. **Roaring Bitmaps** - adaptive compression for real-world distributions
5. **post_filter separates** hits from aggregations for proper faceted navigation
6. **Distributed faceting is approximate** - shard_size and HLL++ are your tools
7. **Memory = Filesystem Cache** - DocValues live in OS cache, not JVM heap

---

## 10. Implementation Checklist

- [ ] Use `keyword` type for facet fields (not `text`)
- [ ] Enable DocValues (default, but verify)
- [ ] Use `filter` context for constraints (not `must`)
- [ ] Implement `post_filter` for faceted navigation UX
- [ ] Set appropriate `shard_size` for distributed accuracy
- [ ] Use `composite` aggregation for pagination
- [ ] Monitor JVM heap vs OS cache ratio
- [ ] Consider HLL++ for high-cardinality counting
