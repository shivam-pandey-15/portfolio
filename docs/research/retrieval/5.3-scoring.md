# Research: Chapter 5.3 - Scoring (BM25)

## Core Concept
Boolean retrieval gives us a set of documents: `{Doc1, Doc2, Doc3}`.
Scoring assigns a float to each: `{Doc1: 0.9, Doc2: 4.5, Doc3: 0.1}`.
Sort by score descending -> Top K.

**BM25 (Best Match 25)** is the industry standard (default in Lucene, Elasticsearch, Solr). It is a probabilistic model extended from simple TF-IDF.

---

## 1. The Three Components

### A. TF (Term Frequency) - "Saturation"
*   **Intuition:** If a document says "Jeans" 1 time, it's relevant. If it says it 100 times, it's *more* relevant, but not 100x more.
*   **Problem with raw count:** A spammer can just repeat "Jeans Jeans Jeans".
*   **BM25 Solution:** Saturation Curve.
    *   The score grows fast at first, then flattens out (asymptotically approaches a max).
    *   Controlled by parameter **k1** (usually 1.2).
    *   Formula part: `TF / (TF + k1)`

### B. IDF (Inverse Document Frequency) - "Rareness"
*   **Intuition:** "The" appears in 100% of docs. "Jeans" appears in 0.1%.
*   **Mechanism:** Penalize common words. Boost rare words.
*   **Formula:** `log(1 + (N - n + 0.5) / (n + 0.5))`
    *   If a term is in *all* documents, IDF is 0 (or negative in some variants, floored to 0).

### C. Field Length Norm - "Density"
*   **Intuition:**
    *   Doc A (Title): "Blue Jeans" (2 words)
    *   Doc B (Body): 1000 words, mentions "Blue Jeans" twice.
    *   Doc A is a *much* better match. Matches in short fields are more significant.
*   **Mechanism:** Penalize long documents.
*   **Controlled by parameter b** (usually 0.75).
    *   `b = 1`: Full length penalization.
    *   `b = 0`: Ignore length (behave like standard TF-IDF).

---

## 2. The Formula (The "Magic")

For a Query $Q$ containing terms $q_1, q_2...$:

$$
Score(D, Q) = \sum_{i=1}^{n} IDF(q_i) \cdot \frac{TF(q_i, D) \cdot (k_1 + 1)}{TF(q_i, D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{avgdl})}
$$

### Broken Down Plainly
1.  **Sum**: We calculate the score for "Blue" and "Jeans" separately and add them up.
2.  **IDF**: How rare is this term? (The weight).
3.  **TF part**: Examples:
    *   `TF * (k1 + 1)`: The numerator scales with frequency.
    *   The Denominator `TF + ...`: This causes the saturation.
4.  **Length part**: `|D| / avgdl` compare this doc's length to the average.
    *   If Doc is Double the average length -> Penalty increases.

---

## 3. Concrete Example: "Blue Jeans"

**Corpus Stat:**
*   Total Docs (N): 1000
*   Avg Doc Length: 100 words

**Query:** "Blue"

**Doc A:**
*   Length: 10 words (Short)
*   Contains "Blue": 1 time

**Doc B:**
*   Length: 1000 words (Long)
*   Contains "Blue": 10 times

**Which wins?**
*   **Doc A:**
    *   TF = 1.
    *   Length Ratio = 10/100 = 0.1 (Very dense).
    *   The `b` term makes the denominator small -> Score is High.
*   **Doc B:**
    *   TF = 10.
    *   Length Ratio = 1000/100 = 10.0 (Diluted).
    *   The `b` term makes the denominator huge -> Score is dampened.
    *   Even though it has 10x occurrences, the 100x length penalty crushes it.

**Result:** Doc A likely wins (or compares favorably), matching human intuition.

---



## 4. Visualizer Ideas for Page

1.  **Saturation Slider:**
    *   Input: `k1` slider (0.5 to 3.0).
    *   Chart: TF vs Score. Show how the curve changes shape.
    *   "See how higher k1 rewards spamming the word more."

2.  **Length Normalization Playground:**
    *   Inputs: Doc Length, Term Count, `b` parameter.
    *   Output: Calculated Score.
    *   "See how a long document needs WAY more matches to compete with a short one."

3.  **The Calculator:**
    *   Interactive Formula. Plug in numbers, see the result.

---

## 5. Defaults Matter
*   **k1 = 1.2**: Empirical "sweet spot" for general search.
*   **b = 0.75**: Penalizes length, but not fully.

---

## 6. Lucene Implementation Details
*   Scores are usually calculated as floats.
*   `Similarity` class in Lucene.
*   Since Lucene 6/7, BM25 is the default (replaced TF-IDF).
*   Stored in `.nvd` / `.nvm` files (Norms).
