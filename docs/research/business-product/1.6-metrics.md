# Research: 1.6 Defining Success Metrics for Search

## The Challenge
How do you know if your search is good? It's not as simple as "users find what they want."

---

## Offline vs Online Metrics

### Offline Metrics
Measured on a fixed dataset, before deployment.
- Used for model development
- Doesn't require production traffic
- Doesn't capture real user behavior

### Online Metrics
Measured on live traffic, after deployment.
- Reflects real user satisfaction
- Requires A/B testing infrastructure
- More expensive to measure

---

## Offline Metrics

### 1. Precision@K
**What it measures**: Of the top K results, how many are relevant?

**Formula**: Relevant in top K / K

**Example**:
- Query: "iPhone case"
- Top 5 results: 4 are iPhone cases, 1 is a screen protector
- Precision@5 = 4/5 = 0.80

**When to use**: When false positives are costly (showing wrong items)

---

### 2. Recall@K
**What it measures**: Of all relevant items, how many are in top K?

**Formula**: Relevant in top K / Total relevant

**Example**:
- Query: "iPhone case"
- 100 iPhone cases in catalog
- Top 50 results contain 40 of them
- Recall@50 = 40/100 = 0.40

**When to use**: When you want to ensure all relevant items are surfaced

---

### 3. NDCG (Normalized Discounted Cumulative Gain)
**What it measures**: Quality of ranking, giving more weight to top positions.

**Intuition**: 
- Relevant item at position 1 is better than at position 10
- Highly relevant item is better than somewhat relevant

**Formula (simplified)**:
```
DCG = Σ (relevance_i / log2(i + 1))
NDCG = DCG / Ideal DCG
```

**Range**: 0 to 1 (1 is perfect ranking)

**When to use**: When order matters, not just presence in results

---

### 4. MRR (Mean Reciprocal Rank)
**What it measures**: How high is the first relevant result?

**Formula**: 1 / position of first relevant result, averaged across queries

**Example**:
- Query 1: First relevant at position 2 → 1/2 = 0.5
- Query 2: First relevant at position 1 → 1/1 = 1.0
- Query 3: First relevant at position 5 → 1/5 = 0.2
- MRR = (0.5 + 1.0 + 0.2) / 3 = 0.57

**When to use**: When user typically only wants one result (navigational queries)

---

## Online Metrics

### 1. Click-Through Rate (CTR)
**What it measures**: % of searches that result in a click

**Formula**: Clicks / Searches

**Benchmarks**:
- E-commerce: 30-50%
- Web search: 20-40%
- Enterprise: 50-70%

**Caveats**:
- High CTR doesn't mean satisfaction (pogo-sticking)
- Low CTR can be good (answer in snippet)
- Position bias (higher results get more clicks)

---

### 2. Zero Result Rate (ZRR)
**What it measures**: % of searches with no results

**Formula**: Searches with 0 results / Total searches

**Benchmarks**:
- Bad: > 10%
- Average: 5-10%
- Good: < 5%
- Excellent: < 2%

**Action**: Review top zero-result queries, add synonyms/fallbacks

---

### 3. Reformulation Rate
**What it measures**: % of searches followed by another search

**Interpretation**:
- High reformulation = user didn't find what they wanted first time
- But: some reformulation is natural (narrowing down)

**Benchmarks**:
- Bad: > 30%
- Average: 15-30%
- Good: < 15%

---

### 4. Time to First Click
**What it measures**: How long until user clicks a result

**Interpretation**:
- Fast click = user found what they wanted quickly
- But: very fast click might mean user is just guessing

**Benchmarks**:
- Good: < 5 seconds
- Average: 5-15 seconds
- Bad: > 15 seconds or no click

---

### 5. Conversion Rate (from search)
**What it measures**: % of searches that lead to purchase/action

**Formula**: Conversions from search / Searches

**Benchmarks** (e-commerce):
- Average: 2-5%
- Good: 5-10%
- Excellent: > 10%

---

### 6. Revenue Per Search
**What it measures**: Average revenue generated per search

**Formula**: Total revenue from search sessions / Total searches

**Use**: North star metric for e-commerce search teams

---

## Search-Specific Metrics

### Dwell Time
Time spent on clicked result before returning to search.
- Long dwell = found useful content
- Short dwell (pogo-sticking) = wrong result

### Scroll Depth
How far user scrolls down the search results page.
- High scroll = looking for answer, might not be at top
- Low scroll = found it quickly OR gave up

### Filter Usage
% of searches that use filters (facets).
- High filter usage = category browsing
- Low filter usage = precise queries

### Autocomplete Acceptance
% of searches that used autocomplete suggestion.
- High = good autocomplete
- Low = users typing full queries themselves

---

## Metric Traps

### Goodhart's Law
"When a measure becomes a target, it ceases to be a good measure."

**Example**: Optimize for CTR → Show clickbait titles → Users click but don't convert

### Vanity Metrics
Metrics that look good but don't matter.

**Example**: "We have 1M searches per day!" (But most return garbage)

### Missing Context
Same metric, different meaning.

**Example**: 
- 0% zero result rate could mean great synonym coverage
- OR it could mean you're returning irrelevant results instead of admitting "no match"

---

## Metric Dashboard

Recommended metrics to track:

### Daily Dashboard
| Metric | Target | Alert If |
|--------|--------|----------|
| Zero Result Rate | < 5% | > 8% |
| CTR | > 35% | < 25% |
| P99 Latency | < 200ms | > 500ms |
| Searches | baseline | -20% |

### Weekly Review
| Metric | Focus |
|--------|-------|
| Top zero-result queries | Add synonyms |
| Low CTR queries | Check ranking |
| Slow queries | Optimize or cache |
| Conversion by query type | Business opportunities |

---

## Key Takeaways

1. **Offline metrics** (NDCG, Precision) for development, **Online metrics** (CTR, ZRR) for production
2. **Zero Result Rate is the first metric to fix** — direct revenue loss
3. **CTR can be misleading** — high CTR + low conversion = clickbait
4. **Use multiple metrics together** — no single metric tells the whole story
5. **Set alerts for anomalies** — catch regressions early
