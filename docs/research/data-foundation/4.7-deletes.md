# 3.7 Deletes & Reindexing

Deletes are the hidden complexity in search systems. "Just delete it" is never simple in a distributed, eventually consistent index.

---

## 1. Why Deletes Are Hard

### The Immutability Problem

Lucene segments are **immutable**. You cannot modify bytes on disk.

```
What you think happens:
Document deleted → Bytes removed from disk

What actually happens:
Document deleted → Bit set in .del file
                   Document still on disk!
                   Just marked as "skip this"
```

### The Distributed Problem

```
Cluster state:
- 3 nodes
- 5 shards
- 1 replica each
- Total: 10 shard copies

Delete request:
- Goes to primary shard
- Forwarded to replica
- Network partition happens...
- Primary thinks deleted
- Replica doesn't know
- Data inconsistency!
```

---

## 2. The Tombstone Tax

### Performance Impact

**How search works with deletes:**
```
Query: "laptop"

Step 1: Find matching docs from inverted index
        Candidates: [1, 4, 7, 12, 15, 23, 45, 67]

Step 2: Check .del bitset for each candidate
        Deleted: [4, 15, 45]
        Remaining: [1, 7, 12, 23, 67]

Step 3: Score and rank remaining docs
```

**The cost:**
```
Segments with 0% deletes: Query time = 50ms
Segments with 20% deletes: Query time = 60ms (+20%)
Segments with 50% deletes: Query time = 85ms (+70%)
```

### Storage Impact

```
1M documents indexed
500K documents deleted

Storage:
- Live docs: 2GB (50%)
- Deleted tombstones: 2GB (50%)
- Total: 4GB

You're paying for data you don't use!
```

### Reclaiming Space: Merge

```
Tombstone cleanup only happens during segment merge:

Before merge:
Segment A: 100K live, 50K deleted (150K total)
Segment B: 80K live, 30K deleted (110K total)

After merge:
Segment AB: 180K live, 0 deleted
```

---

## 3. Delete Strategies

### Strategy 1: Hard Delete

```python
# Direct delete
es.delete(index="products", id="prod_123")
```

**Pros:**
- Simple API
- Immediate visibility (after refresh)

**Cons:**
- Tombstone accumulation
- Cannot recover
- CDC might not capture

### Strategy 2: Soft Delete (Recommended)

```python
# Mark as deleted
es.update(
    index="products",
    id="prod_123",
    doc={
        "is_deleted": True,
        "deleted_at": datetime.now().isoformat()
    }
)

# All queries filter out deleted
{
  "query": {
    "bool": {
      "must": { "match": { "title": "laptop" } },
      "must_not": { "term": { "is_deleted": True } }
    }
  }
}
```

**Pros:**
- Recoverable (undelete)
- Auditable (when deleted)
- No immediate tombstone

**Cons:**
- Storage cost (deleted docs stay)
- Query filter overhead

**Cleanup job:**
```python
# Weekly cleanup: Hard delete docs soft-deleted > 30 days ago
def purge_old_deletes():
    es.delete_by_query(
        index="products",
        body={
            "query": {
                "bool": {
                    "must": [
                        { "term": { "is_deleted": True } },
                        { "range": { "deleted_at": { "lt": "now-30d" } } }
                    ]
                }
            }
        }
    )
```

### Strategy 3: Time-Based Indices (Best for Logs)

```
Index naming:
logs-2024-01-01
logs-2024-01-02
logs-2024-01-03
...

Alias spans all:
logs → logs-2024-01-*, logs-2024-02-*

Deletion = drop entire index:
DELETE logs-2024-01-01

Time: 50ms (metadata operation)
CPU: ~0
Tombstones: 0
```

**Why this works:**
- Index deletion is atomic
- No tombstone accumulation
- No merge overhead
- File system just unlinks files

---

## 4. The Reindexing Problem

### When You Must Reindex

| Change | Requires Reindex? |
|--------|-------------------|
| Adding new field | No (dynamic mapping) |
| Changing field type | **YES** |
| Changing analyzer | **YES** |
| Changing tokenizer | **YES** |
| Changing shard count | **YES** |
| Updating synonyms (index-time) | **YES** |

### The Naive Approach (DON'T DO THIS)

```python
# WRONG: Causes downtime
def reindex_naive():
    es.indices.delete("products")      # Site goes down!
    es.indices.create("products", MAPPING)
    bulk_index_all_products()          # 5 hours later...
    # Site comes back up
```

**Result:** 5 hours of "No search results"

---

## 5. Zero-Downtime Reindexing

### The Alias Pattern

```
Step 1: Current State
┌─────────────────┐
│ Alias: products │ ──────► products_v1 (live index)
└─────────────────┘

Step 2: Create New Index
┌─────────────────┐
│ Alias: products │ ──────► products_v1 (live)
└─────────────────┘
                           products_v2 (new, empty)

Step 3: Copy Data
POST _reindex
{
  "source": { "index": "products_v1" },
  "dest": { "index": "products_v2" }
}

Step 4: Atomic Switch
POST _aliases
{
  "actions": [
    { "remove": { "index": "products_v1", "alias": "products" } },
    { "add": { "index": "products_v2", "alias": "products" } }
  ]
}
// Single atomic operation - no downtime!

Step 5: Cleanup
DELETE products_v1
```

### Handling Updates During Reindex

**The Gap Problem:**
```
T0: Start reindex (products_v1 → v2)
T0-T5h: Reindex running
T1: Customer updates product_123 (written to v1)
T5h: Reindex complete
T5h: Switch alias

Result: v2 is missing the T1 update!
```

**Solution 1: Dual Write**
```python
# Application writes to both during reindex
def update_product(product_id, data):
    es.index(index="products_v1", id=product_id, document=data)
    
    if REINDEX_IN_PROGRESS:
        es.index(index="products_v2", id=product_id, document=data)
```

**Solution 2: Catch-Up Reindex**
```python
def reindex_with_catchup():
    # Record starting point
    start_seq_no = get_latest_seq_no("products_v1")
    
    # Main reindex
    reindex("products_v1", "products_v2")
    
    # Catch-up: Only docs modified after start
    reindex(
        source={
            "index": "products_v1",
            "query": {
                "range": { "_seq_no": { "gt": start_seq_no } }
            }
        },
        dest={"index": "products_v2"}
    )
```

---

## 6. Reindex Performance

### Calculating Reindex Time

```
Variables:
- Document count: 100M
- Average doc size: 2KB
- Total data: 200GB
- Reindex throughput: 5,000 docs/sec

Time = 100M / 5,000 = 20,000 seconds = 5.5 hours
```

### Optimizing Reindex Speed

```json
// Target index settings during reindex
PUT products_v2/_settings
{
  "number_of_replicas": 0,           // No replication during load
  "refresh_interval": "-1"           // No refresh during load
}

// Reindex with slicing (parallelism)
POST _reindex?slices=auto
{
  "source": { "index": "products_v1" },
  "dest": { "index": "products_v2" }
}
// Uses 1 slice per shard, parallel processing

// After reindex
PUT products_v2/_settings
{
  "number_of_replicas": 1,
  "refresh_interval": "1s"
}
```

**Performance gains:**
| Optimization | Speed Improvement |
|--------------|-------------------|
| 0 replicas | +50% |
| Disabled refresh | +30% |
| Parallel slices | +200-400% |
| Larger batch size | +10-20% |
| **Combined** | **+500%+** |

---

## 7. Delete-by-Query Pitfalls

### The Expensive Operation

```json
// This looks simple...
POST products/_delete_by_query
{
  "query": {
    "range": { "created_at": { "lt": "2023-01-01" } }
  }
}

// But internally:
// 1. Execute query (find all matching docs)
// 2. Delete each doc one by one
// 3. Each delete = tombstone
// 4. Cluster-wide coordination
```

**Real example:**
```
Delete 10M old docs from 100M index

Time: 2-4 hours (vs. 50ms for index delete)
CPU: 100% on all nodes
I/O: Massive write amplification
Tombstones: 10M (need merge to clean up)
```

### Better Alternatives

**If data is time-based:**
```
Use time-based indices + index deletion
```

**If data must coexist:**
```
1. Soft delete with is_deleted flag
2. Background job to hard delete in batches
3. Schedule during low-traffic periods
```

---

## 8. Monitoring Deletes

### Key Metrics

```json
// Check deleted docs ratio
GET products/_stats/docs

{
  "primaries": {
    "docs": {
      "count": 8000000,          // Live docs
      "deleted": 2000000         // Tombstones (20% overhead!)
    }
  }
}
```

### Alert Thresholds

| Metric | Warning | Critical |
|--------|---------|----------|
| Delete ratio | > 10% | > 25% |
| Segment count | > 100 | > 500 |
| Max segment age | > 7 days | > 30 days |

### Remediation

```json
// Force merge to clean up tombstones
POST products/_forcemerge?max_num_segments=1&only_expunge_deletes=true

// WARNING: Only run on read-heavy or idle indices!
// Force merge blocks all other operations
```
