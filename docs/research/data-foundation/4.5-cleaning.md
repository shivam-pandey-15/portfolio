# 3.5 Cleaning & Normalization

Data cleaning is the invisible work that determines whether search works or fails. Without normalization, "iPhone", "iphone", "IPHONE", and "i-phone" are four different products.

---

## 1. Why Normalization Matters

### The Problem: Vocabulary Mismatch

**User searches:** `"iphone 15 pro max case"`

**Your data has:**
```json
{ "title": "Case for iPhone 15 Pro Max" }           // "iPhone" not "iphone"
{ "title": "IPHONE 15 PRO MAX Protective Cover" }   // ALL CAPS
{ "title": "i-Phone 15 Pro Max Shell" }             // Hyphenated
{ "title": "Funda para iPhone 15 Pro Max" }         // Spanish
```

**Without normalization:** 0 of 4 match "iphone"
**With normalization:** All 4 match

---

## 2. The Analysis Pipeline

```
Raw Text
    │
    ▼
┌─────────────────┐
│ Character Filter │  ← HTML removal, symbol mapping
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│   Tokenizer     │  ← Split into terms
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  Token Filters  │  ← Lowercase, stemming, synonyms
└────────┬────────┘
         │
         ▼
   Indexed Terms
```

---

## 3. Character Filters (Pre-Processing)

### A. HTML Stripping

**Input:**
```html
<div class="product-info">
  <p>Amazing <b>laptop</b> with <span style="color:red">16GB RAM</span></p>
</div>
```

**Without HTML strip:**
```
Tokens: ["div", "class", "product", "info", "p", "amazing", "b", "laptop", 
         "span", "style", "color", "red", "16gb", "ram", "p", "div"]
```

**With HTML strip:**
```
Tokens: ["amazing", "laptop", "16gb", "ram"]
```

**Configuration:**
```json
{
  "char_filter": {
    "html_strip": {
      "type": "html_strip",
      "escaped_tags": ["b", "i"]  // Keep bold/italic for emphasis
    }
  }
}
```

---

### B. Symbol Mapping

**Problem patterns:**
```
C++     → Standardly tokenized as: ["c"]
C#      → Standardly tokenized as: ["c"]
node.js → Standardly tokenized as: ["node", "js"]
.NET    → Standardly tokenized as: ["net"]
```

**Solution: Pre-tokenization mapping**
```json
{
  "char_filter": {
    "programming_languages": {
      "type": "mapping",
      "mappings": [
        "C++ => cpp",
        "C# => csharp",
        ".NET => dotnet",
        "node.js => nodejs"
      ]
    }
  }
}
```

**Result:**
```
"Senior C++ Developer" → Tokens: ["senior", "cpp", "developer"]
User searches "cpp" → Matches!
```

---

### C. Zero-Width Character Removal

**The Invisible Problem:**
```python
# These look identical but aren't the same:
"iPhone" (normal)
"iPhone" (has zero-width joiner after 'i')

# Byte comparison:
bytes("iPhone") = [105, 80, 104, 111, 110, 101]
bytes("i\u200bPhone") = [105, 226, 128, 139, 80, 104, 111, 110, 101]
```

**Detection:**
```python
def has_invisible_chars(text):
    invisible = [
        '\u200b',  # Zero-width space
        '\u200c',  # Zero-width non-joiner
        '\u200d',  # Zero-width joiner
        '\ufeff',  # Byte order mark
        '\u00ad',  # Soft hyphen
    ]
    return any(char in text for char in invisible)
```

**Cleaning:**
```json
{
  "char_filter": {
    "remove_invisible": {
      "type": "pattern_replace",
      "pattern": "[\\u200b\\u200c\\u200d\\ufeff\\u00ad]",
      "replacement": ""
    }
  }
}
```

---

## 4. Tokenizers

### A. Standard Tokenizer (Default)

**Behavior:**
```
"Quick-brown fox's" → ["Quick", "brown", "fox's"]
"user@email.com"    → ["user", "email.com"]  
"192.168.1.1"       → ["192.168.1.1"]
"$19.99"            → ["19.99"]
```

**Use for:** General text content

---

### B. Whitespace Tokenizer

**Behavior:**
```
"Quick-brown fox's" → ["Quick-brown", "fox's"]
"user@email.com"    → ["user@email.com"]
```

**Use for:** When you need to preserve special punctuation

---

### C. Pattern Tokenizer

**Custom splitting:**
```json
{
  "tokenizer": {
    "sku_tokenizer": {
      "type": "pattern",
      "pattern": "[-_]"  // Split on dash or underscore
    }
  }
}

// Input: "ABC-123-XYZ"
// Tokens: ["ABC", "123", "XYZ"]
```

---

### D. N-Gram Tokenizer (For Autocomplete)

**Configuration:**
```json
{
  "tokenizer": {
    "autocomplete": {
      "type": "edge_ngram",
      "min_gram": 2,
      "max_gram": 10,
      "token_chars": ["letter", "digit"]
    }
  }
}
```

**Input:** "MacBook"
**Tokens:** ["Ma", "Mac", "MacB", "MacBo", "MacBoo", "MacBook"]

**The Cost:**
```
Word length: 10 characters
N-gram (2-10): 9 tokens per word

Document with 100 words of avg 8 chars:
- Without ngram: 100 tokens
- With ngram: 700 tokens (7x storage)
```

---

## 5. Token Filters

### A. Lowercase

**Simple but critical:**
```
"MacBook Pro" → ["macbook", "pro"]
"URGENT: BUY NOW" → ["urgent", "buy", "now"]
```

### B. ASCII Folding

**Handles accents and special characters:**
```
"café"     → "cafe"
"naïve"    → "naive"
"Zürich"   → "zurich"
"北京"     → "北京" (unchanged - only ASCII equivalents)
```

**Configuration:**
```json
{
  "filter": {
    "ascii_folding": {
      "type": "asciifolding",
      "preserve_original": true  // Keep both "café" and "cafe"
    }
  }
}
```

### C. Stemming (Algorithmic)

**Porter Stemmer behavior:**
```
"running"   → "run"
"runs"      → "run"
"runner"    → "runner"  // Inconsistent!

"university" → "univers"
"universe"   → "univers"  // Over-stemmed! Different concepts.

"flies"     → "fli"
"flying"    → "fly"      // Inconsistent!
```

**When to use:** General text search where recall > precision

### D. Lemmatization (Dictionary-Based)

**More accurate but slower:**
```
"better"    → "good"
"went"      → "go"
"feet"      → "foot"
```

**Requirements:**
- Language-specific dictionary
- 20-100MB RAM per language
- 5-10x slower than stemming

### E. Stop Words

**Default English stop words:**
```
a, an, and, are, as, at, be, but, by, for, if, in, into, is, it, 
no, not, of, on, or, such, that, the, their, then, there, these, 
they, this, to, was, will, with
```

**The Debate:**
```
Query: "to be or not to be"
With stop words removed: "" (empty!)

Query: "the who" (band name)
With stop words removed: "who"
Matches: "who is the best?" - wrong!
```

**Modern recommendation:**
```json
// DON'T remove stop words at index time
// DO reduce their weight in scoring

{
  "similarity": {
    "custom_bm25": {
      "type": "BM25",
      "k1": 1.2,
      "b": 0.75
    }
  }
}
// BM25 naturally reduces weight of common terms
```

---

## 6. Semantic Normalization

### A. Unit Normalization

**The Problem:**
```json
{ "ram": "8 GB" }
{ "ram": "8GB" }
{ "ram": "8192 MB" }
{ "ram": "8192MB" }
{ "ram": "8 Gigabytes" }
```

**The Solution: Normalize at ingestion**
```python
import re

def normalize_memory(value):
    """Convert any memory string to MB integer"""
    value = value.upper().replace(" ", "")
    
    patterns = [
        (r"(\d+)TB", lambda m: int(m.group(1)) * 1024 * 1024),
        (r"(\d+)GB", lambda m: int(m.group(1)) * 1024),
        (r"(\d+)MB", lambda m: int(m.group(1))),
        (r"(\d+)GIGABYTES?", lambda m: int(m.group(1)) * 1024),
    ]
    
    for pattern, converter in patterns:
        match = re.search(pattern, value)
        if match:
            return converter(match)
    
    raise ValueError(f"Cannot parse memory: {value}")

# All variations → 8192 (MB)
```

### B. Entity Resolution

**The Problem:**
```json
{ "brand": "Nike" }
{ "brand": "NIKE" }
{ "brand": "Nike, Inc." }
{ "brand": "Nike Corporation" }
{ "brand": "Nike Sportswear" }
```

**The Solution: Canonical IDs**
```python
BRAND_MAPPING = {
    "nike": "brand_nike_001",
    "nike inc": "brand_nike_001",
    "nike corporation": "brand_nike_001",
    "nike sportswear": "brand_nike_001",
    "adidas": "brand_adidas_001",
    "adidas originals": "brand_adidas_001",
}

def resolve_brand(brand_text):
    normalized = brand_text.lower().strip()
    return BRAND_MAPPING.get(normalized, create_new_brand_id(brand_text))
```

**Indexed document:**
```json
{
  "brand_id": "brand_nike_001",      // For filtering/aggregation
  "brand_display": "Nike"            // For display
}
```

---

## 7. Complete Analyzer Example

```json
{
  "settings": {
    "analysis": {
      "char_filter": {
        "html_strip": {
          "type": "html_strip"
        },
        "symbol_mapping": {
          "type": "mapping",
          "mappings": ["C++ => cpp", "C# => csharp"]
        }
      },
      "tokenizer": {
        "standard": {
          "type": "standard"
        }
      },
      "filter": {
        "english_stop": {
          "type": "stop",
          "stopwords": "_english_"
        },
        "english_stemmer": {
          "type": "stemmer",
          "language": "english"
        },
        "custom_synonyms": {
          "type": "synonym",
          "synonyms": [
            "laptop, notebook, portable computer",
            "phone, mobile, smartphone, cell phone"
          ]
        }
      },
      "analyzer": {
        "product_analyzer": {
          "type": "custom",
          "char_filter": ["html_strip", "symbol_mapping"],
          "tokenizer": "standard",
          "filter": [
            "lowercase",
            "asciifolding",
            "english_stop",
            "english_stemmer",
            "custom_synonyms"
          ]
        }
      }
    }
  }
}
```

---

## 8. Testing Your Analyzer

```json
// Analyze API
POST /products/_analyze
{
  "analyzer": "product_analyzer",
  "text": "The <b>Amazing</b> MacBook Pro's display is café-quality!"
}

// Response
{
  "tokens": [
    { "token": "amaz", "position": 0 },
    { "token": "macbook", "position": 1 },
    { "token": "notebook", "position": 1 },  // Synonym!
    { "token": "pro", "position": 2 },
    { "token": "displai", "position": 3 },   // Stemmed
    { "token": "cafe", "position": 4 },      // ASCII folded
    { "token": "qualiti", "position": 5 }    // Stemmed
  ]
}
```
