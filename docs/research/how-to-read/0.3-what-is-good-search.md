# Research: 0.3 What "Good Search" Actually Means

## The Naive Definition (Wrong)
> "Good search returns documents that contain the query terms."

This definition fails immediately:
- Query: "running shoes"
- Result: A blog post titled "Why I stopped running shoes are bad for you"
  - Contains all terms. Completely irrelevant.

---

## The Engineering Definition

### Good Search = Intent Satisfaction

**Formula:**
```
Quality = P(user finds what they want | query, results, context)
```

Components:
1. **Query:** What the user typed
2. **Results:** What we showed
3. **Context:** Who the user is, where they are, what device, time of day

---

## The Three Pillars of Good Search

### 1. Relevance
> Does the result match the user's intent?

**Dimensions:**
- **Textual relevance:** Query terms appear in document
- **Semantic relevance:** Meaning matches (even without exact terms)
- **Business relevance:** The product is available, in budget, shippable

**Example failure:**
- Query: "cheap laptop"
- Result #1: $5000 MacBook Pro
  - Textual match: "laptop" ✓
  - Semantic match: It IS a laptop ✓
  - Business match: User said "cheap" ✗

---

### 2. Speed
> Did the user get results fast enough?

**Thresholds (Google research, Nielsen Norman):**
- < 100ms: Feels instant. User stays in flow.
- 100-300ms: Noticeable delay. Acceptable.
- 300ms - 1s: User notices, may lose patience.
- > 1s: User might abandon. High bounce risk.

**The irony:**
Making search "smarter" (more ML, more reranking) makes it slower.
Good search is the *optimal trade-off*, not maximal intelligence.

---

### 3. Discovery
> Did the user find things they didn't know they wanted?

**Levels:**
1. **Exact match:** "iPhone 15 Pro" → iPhone 15 Pro
2. **Substitute:** "iPhone 15" (out of stock) → "iPhone 14 Pro" (available)
3. **Serendipity:** "running shoes" → "compression socks" (frequently bought together)

**The discovery paradox:**
Users say they want "exactly what they searched for."
But the best search experiences *surprise* them (Netflix, Spotify, Pinterest).

---

## Anti-Patterns: What Bad Search Looks Like

| Symptom | Underlying Problem |
|---------|-------------------|
| "Zero results" for common queries | Missing synonyms, poor tokenization |
| First result is always wrong | Bad ranking features or outdated popularity scores |
| Slow on specific queries | Expensive wildcard/regex, large aggregations |
| Great on laptop, bad in production | Didn't account for cold start, cache misses |
| Users always click result #3 | Result #1 and #2 are ads or irrelevant |

---

## How to Measure "Good"

### Offline Metrics
| Metric | Measures |
|--------|----------|
| Precision@K | % of top K results that are relevant |
| Recall@K | % of all relevant docs in top K |
| NDCG | Quality of ranking order |
| MRR | How high is the first relevant result? |

### Online Metrics
| Metric | Measures |
|--------|----------|
| CTR (Click-Through Rate) | Are users clicking? |
| Zero Result Rate | % of queries with no results |
| Reformulation Rate | % of users who search again (frustration signal) |
| Time to First Click | How long before user finds something? |
| Conversion Rate | Did the search lead to a purchase/action? |

---

## The Ultimate Test

> **If a user searches and leaves without clicking, was your search good or bad?**

Answer: It depends.
- If they saw the answer in the snippet (Google's featured snippets) → Good
- If they saw irrelevant junk and gave up → Bad

This nuance is why search is *hard*.
