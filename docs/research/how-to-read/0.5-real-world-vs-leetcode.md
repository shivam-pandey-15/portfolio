# Research: 0.5 Real-World Search vs LeetCode / Academic IR

## The Gap

### What LeetCode Teaches
- Algorithms on clean, in-memory data structures
- Optimize for time complexity: O(n log n) good, O(n²) bad
- Single machine, deterministic execution
- "Correct" answer exists

### What Real Search Requires
- Distributed systems across 100s of nodes
- Optimize for P99 latency, not average case
- Network failures, partial results, eventual consistency
- "Relevance" is subjective and changes over time

---

## Dimension 1: Data

| Aspect | Academic/LeetCode | Production |
|--------|-------------------|------------|
| Size | Fits in memory | Petabytes across clusters |
| Format | Clean JSON/array | Dirty HTML, PDFs, inconsistent schemas |
| Updates | Static dataset | Real-time streaming, 1000s updates/sec |
| Quality | Perfect | Missing fields, duplicates, spam |

**Real example:**
Academic: "Here's a clean corpus of 10K Wikipedia articles."
Production: "Here's 500M product listings, 20% have broken HTML in descriptions, 
5% are duplicates, and 10K new products are added every hour."

---

## Dimension 2: Correctness

| Aspect | Academic | Production |
|--------|----------|------------|
| Ground truth | Human-labeled relevance | Inferred from clicks (noisy) |
| Evaluation | Precision/Recall on test set | A/B test on real traffic |
| Success | Beat the benchmark | Make more money |

**The click bias problem:**
- Academic: "This document is relevant: Yes/No"
- Production: "User clicked result #2. Does that mean #2 is best, or just that #1 looked boring?"

Click data is the **only** signal at scale, but it's deeply biased:
- Position bias: Higher results get more clicks
- Presentation bias: Results with images get more clicks
- Survivorship bias: Users only click on results you show them

---

## Dimension 3: Latency

| Aspect | Academic | Production |
|--------|----------|------------|
| Constraint | "Runs in reasonable time" | P99 < 50ms |
| Measurement | Wall clock time | Percentile distributions |
| Trade-offs | None (just be fast enough) | Latency vs. quality vs. cost |

**Real constraint:**
- You have 50ms total.
- Network: 10ms
- Retrieval: 15ms
- Ranking: 20ms
- Rendering: 5ms

Your fancy BERT reranker that takes 200ms? **Unusable** without distillation/caching.

---

## Dimension 4: Failure Modes

| Aspect | Academic | Production |
|--------|----------|------------|
| Node failure | Doesn't happen | Daily occurrence |
| Partial results | Not possible | "We got 4 of 5 shards, return best effort" |
| Degradation | Binary (works/broken) | Graceful (disable features under load) |

**Production reality:**
- Primary DC loses power → Failover to secondary in 30 seconds
- One shard is slow → Return partial results, log for debugging
- Black Friday traffic 10x → Disable personalization, serve cached results

---

## Dimension 5: The Feedback Loop

### Academic
Query → Retrieve → Rank → Evaluate (end)

### Production
Query → Retrieve → Rank → Serve → **User interacts** → Log → Retrain model → Deploy → Repeat

**This loop is the moat.**
Teams with good feedback loops improve continuously.
Teams without it are flying blind.

---

## What This Means for You

### If you're coming from LeetCode:
- Your algorithm skills transfer to **inner loops** (scoring functions, data structures)
- You need to learn: distributed systems, observability, experimentation

### If you're coming from ML:
- Your modeling skills matter for **ranking**
- You need to learn: retrieval, latency budgets, where models fit in the pipeline

### If you're coming from backend:
- Your systems skills are the foundation
- You need to learn: IR theory, ranking features, evaluation metrics

---

## The Mental Model Shift

> **LeetCode:** "Find the optimal solution."
> 
> **Production Search:** "Find a good-enough solution that works 99.9% of the time, 
> degrades gracefully the other 0.1%, costs $X/month, and can be improved incrementally."

This guide teaches the second mindset.
