# 3.2 The Inverted Index (Text)

## Plain English Summary

**The inverted index is like a book's index, but way more powerful.**

In a regular book, the content comes first, and the index at the back points to pages. An inverted index does the opposite: it starts with every unique word and lists which documents contain that word.

When you search for "laptop", instead of scanning through millions of product descriptions looking for the word "laptop", the search engine simply looks up "laptop" in the inverted index and instantly gets a list of all matching document IDs.

**Why "inverted"?** Because it inverts the relationship:
- **Normal**: Document → Words it contains
- **Inverted**: Word → Documents that contain it

---

## Real-World Analogy: The Google Docs Search

Imagine you have 10,000 Google Docs and you want to find all documents mentioning "quarterly report".

**Without an inverted index:**
```
Open Doc 1, search for "quarterly report"... not found
Open Doc 2, search for "quarterly report"... found!
Open Doc 3, search for "quarterly report"... not found
... repeat 10,000 times
Time: 3 hours
```

**With an inverted index:**
```
Look up "quarterly" → [Doc 2, Doc 45, Doc 167, Doc 890]
Look up "report" → [Doc 2, Doc 12, Doc 45, Doc 167]
Intersection → [Doc 2, Doc 45, Doc 167]
Time: 0.01 seconds
```

---

## The Core Data Structure

An inverted index has three main components:

### 1. Term Dictionary (The Lookup Table)

This is a sorted list of every unique word (called "terms") in your entire corpus.

```
Term Dictionary:
─────────────────
"apple"
"banana"
"cherry"
"date"
"elderberry"
...
"zebra"
```

**How it's stored: FST (Finite State Transducer)**

Think of an FST like an extremely compressed prefix tree:

```
Traditional storage:
"application" → pointer
"apple" → pointer
"apply" → pointer
"approach" → pointer
= 4 entries × ~15 bytes = 60 bytes

FST storage:
        a-p-p
       /     \
      l       r-o-a-c-h
     / \
    e   i-c-a-t-i-o-n
    |   |
   (*)  y(*)
   
Shared prefixes! = ~25 bytes
```

**The magic:** Looking up a term is O(length of term), NOT O(size of dictionary). Whether you have 1,000 terms or 1 billion terms, looking up "laptop" takes the same time.

### 2. Posting Lists (The Document References)

For each term, we store a list of document IDs that contain it:

```
"laptop"  → [1, 5, 23, 100, 101, 102, 500, 9999]
"macbook" → [5, 23, 500]
"dell"    → [1, 100, 101, 102]
```

**Compression techniques:**

Real posting lists can have millions of entries. We compress them aggressively:

**Delta Encoding:**
```
Original: [1, 5, 23, 100, 101, 102, 500, 9999]
Deltas:   [1, 4, 18, 77, 1, 1, 398, 9499]

The deltas are smaller numbers = fewer bytes to store
```

**Variable-Length Integers (VInt):**
```
Small numbers use fewer bytes:
1-127:     1 byte
128-16383: 2 bytes
etc.

Most deltas are small, so this saves a lot of space.
```

**Roaring Bitmaps (for dense ranges):**
```
Documents 1000-1100 all match:
Instead of: [1000, 1001, 1002, ..., 1100]  (101 integers)
Use bitmap: 111111...1 at offset 1000 (13 bytes)
```

**Real-world compression:**
```
Term: "the" (appears in 50 million documents)

Uncompressed: 50M × 4 bytes = 200 MB
Compressed: ~5 MB (40x smaller!)
```

### 3. Position Data (For Phrase Queries)

To answer phrase queries like `"quick brown fox"`, we need to know WHERE in each document each term appears:

```
"quick" in Doc1: positions [0, 45, 89]
"brown" in Doc1: positions [1, 67]
"fox"   in Doc1: positions [2]

For phrase "quick brown fox":
- "quick" at position 0 ✓
- "brown" at position 1 (= 0 + 1) ✓
- "fox" at position 2 (= 1 + 1) ✓
- Phrase found at position 0!
```

**Position data is expensive:**
```
Index without positions: 100 MB
Index with positions: 800 MB (8x larger!)

Only enable if you need phrase queries or highlighting.
```

---

## Step-by-Step Query Execution

Let's trace exactly what happens when you search:

### Example Query: "wireless keyboard"

**Step 1: Parse the Query**
```
Input: "wireless keyboard"
→ Apply same analysis as indexing
→ Tokens: ["wireless", "keyboard"]
→ Query type: AND (both terms must match)
```

**Step 2: Look Up Each Term in Dictionary**
```
FST lookup "wireless" → found, pointer to posting list A
FST lookup "keyboard" → found, pointer to posting list B
Time: ~100 nanoseconds per term
```

**Step 3: Retrieve Posting Lists**
```
"wireless" → [10, 45, 89, 234, 567, 890, 1234, ...]
"keyboard" → [45, 123, 567, 789, 890, 2345, ...]
```

**Step 4: Intersect the Lists (AND query)**

We need documents that appear in BOTH lists:

```
Algorithm: Two-pointer merge

List A: [10, 45, 89, 234, 567, 890, 1234]
         ^
List B: [45, 123, 567, 789, 890, 2345]
         ^

10 < 45, advance A
45 = 45, MATCH! Output 45, advance both
89 < 123, advance A
234 < 123? No, advance B
234 < 567, advance A
567 = 567, MATCH! Output 567, advance both
890 = 890, MATCH! Output 890, advance both
1234 > 2345? No, advance B
Done.

Result: [45, 567, 890]
```

**Step 5: Score the Matches**

For each matching document, calculate a relevance score (BM25):

```python
def bm25_score(doc, query_terms):
    score = 0
    for term in query_terms:
        tf = term_frequency(term, doc)  # How often term appears
        df = document_frequency(term)    # How many docs have term
        score += idf(df) * tf_weight(tf, doc_length)
    return score
```

**Step 6: Return Top Results**
```
Results (sorted by score):
1. Doc 567: "Logitech Wireless Keyboard MX Keys" (score: 15.4)
2. Doc 890: "Apple Magic Keyboard Wireless" (score: 14.2)
3. Doc 45: "Wireless Keyboard and Mouse Combo" (score: 13.8)
```

**Total time: ~5 milliseconds**

---

## Boolean Operations Deep Dive

### AND (Intersection)

Both terms must be present.

```
"laptop AND macbook" →
laptop:  [1, 5, 23, 100, 500]
macbook: [5, 23, 500]
Result:  [5, 23, 500]
```

**Optimization: Start with the smaller list**

```
If laptop has 10,000 docs and macbook has 100 docs:
- Iterate through macbook's list (100 items)
- For each, check if it exists in laptop's list
- Much faster than the reverse!
```

### OR (Union)

Either term can be present.

```
"laptop OR macbook" →
laptop:  [1, 5, 23, 100, 500]
macbook: [5, 23, 500, 999]
Result:  [1, 5, 23, 100, 500, 999]
```

### NOT (Exclusion)

First term but not second.

```
"laptop NOT gaming" →
laptop: [1, 5, 23, 100, 500]
gaming: [23, 100]
Result: [1, 5, 500]
```

**Warning: "NOT" alone is dangerous**

```
"NOT expensive" = Every document that doesn't contain "expensive"
= Potentially millions of documents!
Always combine with positive terms.
```

### Skip Lists (Performance Optimization)

For large posting lists, we add "skip pointers" to jump ahead:

```
Posting list (1 million docs): [1, 5, 10, 15, 20, ..., 999000, 999500, 1000000]

Skip list (every 1000 elements):
Level 1: [1] ─────► [1000] ─────► [2000] ─────► ...
Level 0: [1, 5, 10, ..., 995, 1000, 1005, ..., 1995, 2000, ...]

To find doc 999750:
1. Level 1: Jump to 999000
2. Level 0: Scan from 999000 to 999750

Skipped: 999,000 comparisons!
```

---

## The Analysis Pipeline

Before text enters the inverted index, it goes through analysis:

```
Raw Text
    │
    ▼
┌─────────────────────────────────────────┐
│          CHARACTER FILTERS              │
│                                         │
│  • Strip HTML tags                      │
│  • Map special chars (C++ → cpp)        │
│  • Remove accents (café → cafe)         │
└────────────────────┬────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────┐
│             TOKENIZER                   │
│                                         │
│  "Hello, World!" → ["Hello", "World"]   │
│                                         │
│  Types:                                 │
│  • Standard: Split on whitespace/punct  │
│  • Whitespace: Split only on whitespace │
│  • Pattern: Split on regex              │
└────────────────────┬────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────┐
│           TOKEN FILTERS                 │
│                                         │
│  • Lowercase: "Hello" → "hello"         │
│  • Stemming: "running" → "run"          │
│  • Synonyms: "laptop" → ["laptop",      │
│              "notebook", "portable"]    │
│  • Stop words: Remove "the", "a", "is"  │
└────────────────────┬────────────────────┘
                     │
                     ▼
              Final Terms
         ["hello", "world"]
```

**Why this matters:**

The same analysis must happen at both index time and query time. If your index lowercases "Apple" to "apple", your query for "APPLE" must also become "apple" or it won't match!

---

## Common Pitfalls (and How to Avoid Them)

### Pitfall 1: GUIDs as Text

```json
// BAD
{ "session_id": "550e8400-e29b-41d4-a716-446655440000" }

// Standard analyzer splits on hyphens:
["550e8400", "e29b", "41d4", "a716", "446655440000"]

// Problems:
// 1. 5 terms per document instead of 1
// 2. Searching for the full ID matches partial IDs too!
// 3. Term dictionary explodes
```

**Fix: Use keyword type**
```json
{
  "mappings": {
    "properties": {
      "session_id": { "type": "keyword" }
    }
  }
}
```

### Pitfall 2: Aggregating on Text Fields

```json
// BAD: Aggregating on analyzed text
{
  "aggs": {
    "top_colors": {
      "terms": { "field": "color" }
    }
  }
}

// If color is "type": "text":
// "Light Blue" → ["light", "blue"]
// Results: "light": 5000, "blue": 4000
// Not useful!
```

**Fix: Use keyword subfield**
```json
{
  "mappings": {
    "properties": {
      "color": {
        "type": "text",
        "fields": {
          "keyword": { "type": "keyword" }
        }
      }
    }
  }
}

// Aggregate on color.keyword
{
  "aggs": {
    "top_colors": {
      "terms": { "field": "color.keyword" }
    }
  }
}
// Results: "Light Blue": 5000, "Red": 4000
```

### Pitfall 3: Stop Words in Phrase Queries

```json
// Query: "to be or not to be"

// With stop words removed:
// Tokens: [] (empty!)
// No matches!

// Modern approach: Keep stop words, but lower their scoring weight.
```

---

## Performance Benchmarks

**Dataset: 100 million product descriptions (~50 GB text)**

| Query Type | Time | Notes |
|------------|------|-------|
| Single term | 3ms | Index lookup + score top 1000 |
| AND (2 terms) | 5ms | Intersection of posting lists |
| AND (5 terms) | 12ms | Multiple intersections |
| Phrase (2 words) | 8ms | Check positions |
| Phrase (5 words) | 20ms | Many position checks |
| Wildcard `lap*` | 50ms | Scan term dictionary |
| Wildcard `*top` | 500ms | Must scan entire dictionary! |

**Memory usage:**
```
Raw text data: 50 GB
Term dictionary (FST): 500 MB
Posting lists (compressed): 8 GB
Position data: 25 GB
Total inverted index: ~34 GB (0.68x raw text)
```

---

## Key Takeaways

1. **The inverted index maps terms to documents**, not documents to terms. This is what makes text search fast.

2. **Three components**: Term dictionary (FST), posting lists (compressed), and optional position data.

3. **Query execution** is term lookup + posting list operations (intersection, union).

4. **Analysis matters**: The same analysis must happen at index time and query time.

5. **Use the right field types**: `text` for full-text search, `keyword` for exact matches.

---

## Forward References

- **[3.3 BKD Trees](3.3-bkd-docvalues.md)**: How numeric data is indexed (different structure!)
- **[Ch 4.5 Cleaning](../data-foundation/4.5-cleaning.md)**: Deep dive into the analysis pipeline
