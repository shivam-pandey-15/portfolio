# 3.7 The Write & Query Path

## Plain English Summary

**Understanding the lifecycle of a request reveals where latency comes from and how to optimize it.**

Think of a search engine like a restaurant. When you place an order (write), the kitchen has to prepare it safely and store it properly. When you ask for the menu (query), the waiter checks multiple stations and combines what they find.

The **write path** is how data gets from your application into the searchable index. The **query path** is how a search request finds and returns results.

Understanding these paths is crucial for debugging slow queries, tuning throughput, and building reliable systems.

---

## Real-World Analogy: The Restaurant Kitchen

### Write Path (Placing an Order)

```
Customer: "I'd like the pasta special"

1. RECEIPT (Translog)
   Waiter immediately writes order on paper receipt.
   → Even if kitchen forgets, we have the order recorded.
   → This is DURABLE.

2. KITCHEN BOARD (In-Memory Buffer)
   Order goes on the kitchen's prep board.
   → Kitchen knows what to make.
   → Not yet on a plate.
   → If power goes out, we check the receipt.

3. PLATE (Segment)
   Every few minutes, completed dishes go on plates.
   → Now it can be served to customers!
   → This is SEARCHABLE.

4. PHOTO FOR MENU (Commit)
   Periodically, we photograph everything for the menu.
   → Permanent record.
   → Can throw away receipts.
```

### Query Path (Ordering from Menu)

```
Customer: "What pasta dishes do you have?"

1. MANAGER (Coordinator)
   Receives the question.
   Needs to check ALL kitchen stations.

2. ASK ALL STATIONS (Scatter)
   "Hey pasta station, what's available?"
   "Hey cold station, any pasta salads?"
   Each station reports their options.

3. COMBINE ANSWERS (Merge)
   Manager collects all answers.
   Picks the best ones to present.

4. GET FULL DETAILS (Fetch)
   "OK, tell me the full details of these 3 dishes."
   Get complete information for final presentation.

5. PRESENT TO CUSTOMER (Response)
   "Here are our pasta dishes..."
```

---

## The Write Path: Step by Step

Let's trace exactly what happens when you index a document:

### Step 0: Client Sends Request

```json
POST /products/_doc
{
  "title": "MacBook Pro 16-inch",
  "price": 2499,
  "category": "laptops"
}
```

### Step 1: Coordinator Receives Request (0.1ms)

```
┌────────────────────────────────────────────┐
│           COORDINATOR NODE                  │
│                                            │
│  1. Parse incoming JSON                    │
│  2. If no _id provided, generate UUID      │
│  3. Calculate target shard:                │
│     shard = hash(_id) % num_shards         │
│  4. Forward to primary shard owner         │
│                                            │
│  Time: ~0.1ms                              │
└────────────────────────────────────────────┘
```

### Step 2: Network Hop to Primary Shard (0.5ms)

```
If coordinator is NOT the primary shard owner:
  Send request over network → 0.5ms typical
  
If coordinator IS the primary shard owner:
  Local call → 0.01ms
```

### Step 3: Write to Translog (1-5ms) ⭐ CRITICAL

```
┌────────────────────────────────────────────┐
│           TRANSLOG (DISK)                  │
│                                            │
│  Append-only file on disk:                 │
│                                            │
│  [Previous transactions...]               │
│  [Doc: MacBook Pro, $2499, laptops]  ← NEW│
│                                            │
│  Then: fsync() to disk                     │
│                                            │
│  Time: 1-5ms (SSD), 10-20ms (HDD)         │
│                                            │
│  ════════════════════════════════════════  │
│  DATA IS NOW DURABLE                       │
│  (Survives crash, power loss)              │
└────────────────────────────────────────────┘
```

**Why translog first?**
- Appending is extremely fast (sequential I/O)
- If we crash before creating segments, we can replay translog
- fsync ensures data is physically on disk

### Step 4: Add to In-Memory Buffer (0.5ms)

```
┌────────────────────────────────────────────┐
│        IN-MEMORY BUFFER (RAM)              │
│                                            │
│  Analysis happens:                         │
│  "MacBook Pro 16-inch"                     │
│  → Tokenize: [macbook, pro, 16, inch]      │
│  → Build in-memory index structures        │
│                                            │
│  Time: ~0.5ms                              │
│                                            │
│  ════════════════════════════════════════  │
│  DATA IS NOT YET SEARCHABLE                │
│  (Only visible after refresh)              │
└────────────────────────────────────────────┘
```

### Step 5: Replicate to Replica Shards (1-5ms)

```
┌──────────────────────────────────────────────────────────────┐
│                      REPLICATION                              │
│                                                               │
│  Primary Shard ───────► Replica Shard 1                      │
│       │                      │                                │
│       │                      ▼                                │
│       │            [Writes to its translog]                  │
│       │            [Adds to its buffer]                      │
│       │                      │                                │
│       │                      ▼                                │
│       ◄─────────────────── ACK                               │
│                                                               │
│  Configurable: wait_for_active_shards                        │
│  - "1": Just primary (fast, risky)                           │
│  - "all": All replicas (slow, safe)                          │
│  - "quorum": Majority (balanced)                             │
│                                                               │
│  Time: 1-5ms (network round trip)                            │
└──────────────────────────────────────────────────────────────┘
```

### Step 6: Return Success to Client

```
Total latency breakdown:
- Coordinator routing: 0.1ms
- Network to primary: 0.5ms  
- Translog write + fsync: 3ms
- In-memory buffer: 0.5ms
- Replication: 2ms
────────────────────────────
TOTAL: ~6ms

Response:
{
  "_id": "abc123",
  "_version": 1,
  "result": "created"
}
```

### Step 7: Refresh (Async, Every 1 Second)

```
┌────────────────────────────────────────────┐
│              REFRESH                        │
│         (Background, async)                 │
│                                            │
│  In-Memory Buffer → New Segment File       │
│                                            │
│  The segment contains:                     │
│  - Inverted index (terms → docs)           │
│  - BKD trees (numeric ranges)              │
│  - DocValues (sorting data)                │
│  - Stored fields (_source)                 │
│                                            │
│  ════════════════════════════════════════  │
│  DATA IS NOW SEARCHABLE!                   │
│                                            │
│  Time: 10-100ms                            │
│  Frequency: Every 1 second (default)       │
└────────────────────────────────────────────┘
```

### Step 8: Flush (Async, Periodically)

```
┌────────────────────────────────────────────┐
│                FLUSH                        │
│         (Background, async)                 │
│                                            │
│  1. Ensure all segments are fsync'd        │
│  2. Write "commit point" marker            │
│  3. Clear translog (no longer needed)      │
│                                            │
│  Triggers:                                 │
│  - Every 30 minutes                        │
│  - Translog exceeds 512MB                  │
│  - Manual: POST /index/_flush              │
│                                            │
│  ════════════════════════════════════════  │
│  DATA IS FULLY COMMITTED                   │
│  (Translog can be deleted)                 │
└────────────────────────────────────────────┘
```

---

## The Query Path: Step by Step

### Step 0: Client Sends Query

```json
GET /products/_search
{
  "query": {
    "bool": {
      "must": [
        { "match": { "title": "laptop" } }
      ],
      "filter": [
        { "range": { "price": { "lte": 2000 } } }
      ]
    }
  },
  "size": 10,
  "sort": [{ "price": "asc" }]
}
```

### Step 1: Coordinator Receives Query (0.2ms)

```
┌────────────────────────────────────────────┐
│           COORDINATOR NODE                  │
│                                            │
│  1. Parse query JSON                       │
│  2. Identify target shards:                │
│     - No routing: ALL shards              │
│     - With routing: Specific shard(s)      │
│  3. Choose which copy of each shard:       │
│     - Round-robin or least-loaded         │
│     - Primary OR replica (either works)    │
│                                            │
│  Time: ~0.2ms                              │
└────────────────────────────────────────────┘
```

### Step 2: Query Phase - Scatter (5-50ms)

```
┌──────────────────────────────────────────────────────────────┐
│                      QUERY PHASE                              │
│                                                               │
│  Coordinator broadcasts to all targeted shard copies:        │
│                                                               │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐          │
│  │  Shard 0    │  │  Shard 1    │  │  Shard 2    │          │
│  │             │  │             │  │             │          │
│  │  1. Look up │  │  1. Look up │  │  1. Look up │          │
│  │    "laptop" │  │    "laptop" │  │    "laptop" │          │
│  │             │  │             │  │             │          │
│  │  2. Check   │  │  2. Check   │  │  2. Check   │          │
│  │    deleted  │  │    deleted  │  │    deleted  │          │
│  │    docs     │  │    docs     │  │    docs     │          │
│  │             │  │             │  │             │          │
│  │  3. Apply   │  │  3. Apply   │  │  3. Apply   │          │
│  │    filter   │  │    filter   │  │    filter   │          │
│  │    (price)  │  │    (price)  │  │    (price)  │          │
│  │             │  │             │  │             │          │
│  │  4. Score   │  │  4. Score   │  │  4. Score   │          │
│  │    (BM25)   │  │    (BM25)   │  │    (BM25)   │          │
│  │             │  │             │  │             │          │
│  │  Returns:   │  │  Returns:   │  │  Returns:   │          │
│  │  Top 10 IDs │  │  Top 10 IDs │  │  Top 10 IDs │          │
│  │  + scores   │  │  + scores   │  │  + scores   │          │
│  └─────────────┘  └─────────────┘  └─────────────┘          │
│                                                               │
│  Note: NO actual document content returned yet!              │
│        Just IDs + scores (lightweight)                       │
│                                                               │
│  Time: 5-50ms (depends on query complexity)                  │
└──────────────────────────────────────────────────────────────┘
```

### Step 3: Merge Results (0.5ms)

```
┌────────────────────────────────────────────┐
│           COORDINATOR MERGE                 │
│                                            │
│  Received from shards:                     │
│                                            │
│  Shard 0: [(doc1, 9.5), (doc2, 8.2), ...]  │
│  Shard 1: [(doc45, 8.8), (doc46, 7.1),...] │
│  Shard 2: [(doc99, 9.2), (doc100, 6.5),...]│
│                                            │
│  Merge using priority queue:               │
│                                            │
│  Global top 10:                            │
│  1. doc1 (9.5) - Shard 0                   │
│  2. doc99 (9.2) - Shard 2                  │
│  3. doc45 (8.8) - Shard 1                  │
│  4. doc2 (8.2) - Shard 0                   │
│  ... etc                                   │
│                                            │
│  Time: ~0.5ms                              │
└────────────────────────────────────────────┘
```

### Step 4: Fetch Phase - Gather (2-10ms)

```
┌──────────────────────────────────────────────────────────────┐
│                      FETCH PHASE                              │
│                                                               │
│  Coordinator knows winning doc IDs and which shards have them│
│                                                               │
│  Only contacts shards with winning documents:                │
│                                                               │
│  ┌─────────────┐              ┌─────────────┐                │
│  │  Shard 0    │              │  Shard 2    │                │
│  │             │              │             │                │
│  │  "Get me    │              │  "Get me    │                │
│  │   doc1,     │              │   doc99"    │                │
│  │   doc2"     │              │             │                │
│  │             │              │             │                │
│  │  Returns:   │              │  Returns:   │                │
│  │  Full JSON  │              │  Full JSON  │                │
│  │  _source    │              │  _source    │                │
│  └─────────────┘              └─────────────┘                │
│                                                               │
│  Shard 1 not contacted (no winning docs from there)          │
│                                                               │
│  Time: 2-10ms                                                │
└──────────────────────────────────────────────────────────────┘
```

### Step 5: Return to Client

```
Total latency breakdown:
- Parse query: 0.2ms
- Query phase: 20ms
- Merge: 0.5ms
- Fetch phase: 5ms
────────────────────────────
TOTAL: ~26ms

Response:
{
  "took": 26,
  "hits": {
    "total": 1500,
    "hits": [
      { "_id": "doc1", "_source": { "title": "MacBook Pro", ... } },
      { "_id": "doc99", "_source": { "title": "Dell XPS", ... } },
      ...
    ]
  }
}
```

---

## Optimistic Concurrency Control (OCC)

### The Problem: Lost Updates

```
Time 0: Document has stock = 100

Thread A:                    Thread B:
  Read: stock = 100            Read: stock = 100
  Compute: 100 - 1 = 99        Compute: 100 - 1 = 99
  Write: stock = 99            Write: stock = 99

Expected: stock = 98
Actual: stock = 99  ← LOST UPDATE!
```

### The Solution: Version Checking

Elasticsearch tracks a sequence number and primary term for each document:

```json
// Read document with version info
GET /products/_doc/123

{
  "_id": "123",
  "_seq_no": 5,
  "_primary_term": 1,
  "_source": { "stock": 100 }
}
```

```json
// Update with version check
PUT /products/_doc/123?if_seq_no=5&if_primary_term=1
{
  "stock": 99
}

// If someone else updated first, we get 409 Conflict!
```

### Retry Pattern

```python
def safe_update(doc_id, update_function, max_retries=3):
    for attempt in range(max_retries):
        # 1. Read current version
        doc = es.get(index="products", id=doc_id)
        seq_no = doc["_seq_no"]
        primary_term = doc["_primary_term"]
        
        # 2. Apply your update logic
        updated_doc = update_function(doc["_source"])
        
        try:
            # 3. Write with version check
            es.index(
                index="products",
                id=doc_id,
                document=updated_doc,
                if_seq_no=seq_no,
                if_primary_term=primary_term
            )
            return  # Success!
            
        except ConflictError:
            # Someone else updated first, retry!
            if attempt == max_retries - 1:
                raise
            time.sleep(0.1 * (2 ** attempt))  # Exponential backoff

# Usage
safe_update("123", lambda doc: {**doc, "stock": doc["stock"] - 1})
```

### Script Updates (Atomic)

For high-contention updates, use scripts that run atomically:

```json
POST /products/_update/123
{
  "script": {
    "source": "ctx._source.stock -= params.quantity",
    "params": {
      "quantity": 1
    }
  }
}
```

This runs atomically on the shard leader - no read-modify-write race!

---

## Caching Layers

```
┌──────────────────────────────────────────────────────────────┐
│                     CACHING STACK                             │
│                                                               │
│  ┌─────────────────────────────────────────────────────────┐ │
│  │        NODE QUERY CACHE (per node)                       │ │
│  │                                                          │ │
│  │  Caches: Filter clause results (bitsets of doc IDs)     │ │
│  │  Example: "category = laptops" → [doc1, doc5, doc99...]  │ │
│  │  Invalidates: On segment refresh                         │ │
│  └─────────────────────────────────────────────────────────┘ │
│                                                               │
│  ┌─────────────────────────────────────────────────────────┐ │
│  │       SHARD REQUEST CACHE (per shard)                   │ │
│  │                                                          │ │
│  │  Caches: Full aggregation results                        │ │
│  │  Example: "avg price by category" → {laptops: 1500,...}  │ │
│  │  Invalidates: On refresh                                 │ │
│  └─────────────────────────────────────────────────────────┘ │
│                                                               │
│  ┌─────────────────────────────────────────────────────────┐ │
│  │        FIELDDATA CACHE (per segment)                    │ │
│  │                                                          │ │
│  │  Caches: Global ordinals for string aggregations         │ │
│  │  Example: Brand "Apple" = ordinal 0, "Dell" = ordinal 1  │ │
│  │  Invalidates: Never (per segment, segments are immutable)│ │
│  └─────────────────────────────────────────────────────────┘ │
│                                                               │
│  ┌─────────────────────────────────────────────────────────┐ │
│  │         OS PAGE CACHE (filesystem)                      │ │
│  │                                                          │ │
│  │  Caches: Hot segment files                               │ │
│  │  Managed by: Operating system (not Elasticsearch)        │ │
│  │  Target: Keep entire index in RAM if possible            │ │
│  └─────────────────────────────────────────────────────────┘ │
│                                                               │
└──────────────────────────────────────────────────────────────┘
```

### Cache Hit Rate Targets

| Cache | Healthy Hit Rate |
|-------|------------------|
| Query Cache | 80-95% |
| Request Cache | 50-80% |
| Page Cache | 95%+ |

---

## Debugging Slow Queries

### Profile API

```json
GET /products/_search
{
  "profile": true,
  "query": {
    "match": { "title": "laptop" }
  }
}
```

Response includes timing breakdown:

```json
{
  "profile": {
    "shards": [{
      "searches": [{
        "query": [{
          "type": "TermQuery",
          "description": "title:laptop",
          "time_in_nanos": 1500000,
          "breakdown": {
            "build_scorer": 500000,
            "next_doc": 800000,
            "score": 200000
          }
        }]
      }]
    }]
  }
}
```

### Slow Log

```json
PUT /products/_settings
{
  "index.search.slowlog.threshold.query.warn": "10s",
  "index.search.slowlog.threshold.query.info": "5s",
  "index.search.slowlog.threshold.fetch.warn": "1s"
}
```

Logs all queries exceeding thresholds!

---

## Key Takeaways

1. **Write path**: Client → Coordinator → Primary → Translog (durable!) → Buffer → Replicas → Response. Segment creation is async.

2. **Query path**: Scatter to all shards → Each shard finds local top N → Coordinator merges → Fetch full documents.

3. **Translog is durability**, segments are searchability. Different guarantees!

4. **OCC prevents lost updates**: Use `if_seq_no` and `if_primary_term`, or use scripts for atomic updates.

5. **Multiple caching layers** improve performance, but invalidate on refresh.

6. **Profile API** is your friend for debugging slow queries.

---

## Forward References

- **[Ch 4.6 Freshness](../data-foundation/4.6-freshness.md)**: Tuning the write path for your latency needs
- **[Ch 12 Caching](../caching/12.1-latency.md)**: Advanced caching strategies
