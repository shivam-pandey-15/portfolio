# 3.6 Sharding & Distributed Architecture

## Plain English Summary

**Sharding is how search engines handle more data than fits on one computer.**

Imagine you have a library with 100 million books. One building can't hold them all, so you split them across 10 buildings. Each building is a **shard**. When someone searches, a librarian (the **coordinator**) asks each building to search its portion, then combines the results.

**Replicas** are copies of each shard on different machines, like having backup libraries in case one burns down (or just gets too busy).

This is horizontal scaling: instead of one giant computer, use many normal computers working together.

---

## Real-World Analogy: The Pizza Chain

Imagine you run a pizza delivery chain:

**Single Store (No Sharding):**
```
One store handles ALL orders in the city.

Problems:
- Store gets overwhelmed during peak hours
- If store burns down, no pizza for anyone
- Kitchen too small for 10,000 daily orders
```

**Multiple Stores (Sharded):**
```
City divided into 5 zones, one store per zone.

Order arrives:
1. Call center (coordinator) receives order
2. Routes to correct store based on address (routing)
3. Store makes pizza and delivers

Benefits:
- Each store handles manageable load
- One store down? Others still serve most of city
- Can add more stores as city grows
```

**Backup Stores (Replicas):**
```
Each zone has 2 stores (primary + backup).

If primary store burns down:
- Backup store immediately takes over
- Customers see no interruption

During busy times:
- Both stores can serve customers
- 2x delivery capacity!
```

---

## The Shard Model

### Logical vs Physical View

**What you see (logical):**
```
One "products" index with 100 million documents.

POST /products/_search
{ "query": { "match": { "title": "laptop" } } }
```

**What actually exists (physical):**
```
┌─────────────────────────────────────────────────────────────┐
│                    INDEX: "products"                         │
│                    (Logical name)                            │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│   ┌─────────────┐  ┌─────────────┐  ┌─────────────┐        │
│   │  Shard 0    │  │  Shard 1    │  │  Shard 2    │        │
│   │  33M docs   │  │  33M docs   │  │  34M docs   │        │
│   │  Node A     │  │  Node B     │  │  Node C     │        │
│   └─────────────┘  └─────────────┘  └─────────────┘        │
│        │                │                │                  │
│        ▼                ▼                ▼                  │
│   ┌─────────────┐  ┌─────────────┐  ┌─────────────┐        │
│   │  Shard 0    │  │  Shard 1    │  │  Shard 2    │        │
│   │  (Replica)  │  │  (Replica)  │  │  (Replica)  │        │
│   │  Node B     │  │  Node C     │  │  Node A     │        │
│   └─────────────┘  └─────────────┘  └─────────────┘        │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

**Key insight:** Each shard is a fully independent Lucene index with its own inverted index, BKD trees, segments, etc.

---

## Routing: How Documents Find Their Home

### The Routing Formula

When you index a document, the engine decides which shard it goes to:

```python
shard_number = hash(routing_key) % number_of_shards

# By default, routing_key = document_id
```

**Example:**
```
Document ID: "product_abc123"
Number of shards: 5

Calculation:
  hash("product_abc123") = 2847593847
  2847593847 % 5 = 2
  
→ Document goes to Shard 2
```

**Why this matters:**
- Same document always goes to same shard (deterministic)
- Documents are evenly distributed (hash spreads them out)
- Looking up a document by ID only needs to check ONE shard

### Custom Routing

Sometimes you want control over which documents live together:

```json
// Route all orders for a user to the same shard
POST /orders/_doc?routing=user_456
{
  "user_id": "user_456",
  "item": "laptop",
  "price": 999
}

// Later search only hits ONE shard!
GET /orders/_search?routing=user_456
{
  "query": { "term": { "user_id": "user_456" } }
}
```

**Use cases:**
1. **Multi-tenant**: Route by `tenant_id`
2. **User data**: Route by `user_id`
3. **Parent-child**: Keep parent and children together

**Danger: Hot Spots!**
```
User "power_user_1" has 10 million documents.
All 10M are routed to Shard 2 (because same routing key).

Result:
  Shard 0: 50 MB
  Shard 1: 50 MB
  Shard 2: 5 GB  ← This shard is CRUSHED
  Shard 3: 50 MB
  Shard 4: 50 MB
```

---

## Replication: Availability and Throughput

### Primary vs Replica Shards

```
Write flow:
┌────────┐     ┌────────────┐     ┌────────────┐
│ Client │ ──► │ Coordinator│ ──► │ Primary 0  │ ──┬──► │ Replica 0 │
└────────┘     └────────────┘     │ (Node A)   │   │    │ (Node B)  │
                                  └────────────┘   │    └───────────┘
                                                   │
                                                   └──► (Wait for ack)

Order of operations:
1. Primary assigns sequence number
2. Primary forwards to replica(s)
3. Wait for acknowledgment
4. Return success to client
```

```
Read flow:
┌────────┐     ┌────────────┐     ┌────────────┐
│ Client │ ──► │ Coordinator│ ──► │ ANY copy   │
└────────┘     └────────────┘     │ (Primary   │
                                  │  OR Replica)│
                                  └────────────┘

Read load is distributed across all copies!
With 1 replica: 2x read capacity
With 2 replicas: 3x read capacity
```

### Consistency Levels

You can control how many replicas must acknowledge a write:

```json
// Fast (just primary) - Risk: data loss if primary fails before replication
POST /products/_doc?wait_for_active_shards=1
{ "title": "MacBook Pro" }

// Safe (all replicas) - Slow: wait for all copies
POST /products/_doc?wait_for_active_shards=all
{ "title": "MacBook Pro" }

// Balanced (quorum) - Default in most cases
POST /products/_doc?wait_for_active_shards=quorum
{ "title": "MacBook Pro" }
```

---

## Query Execution: Scatter-Gather

### The Two-Phase Search

When you search, the query goes to ALL shards (or all relevant shards if using routing):

**Phase 1: Query (Scatter)**
```
┌──────────────────────────────────────────────────────────────┐
│                        COORDINATOR                            │
│                                                               │
│  Query: "match title: laptop"                                │
│                                                               │
│  "Hey shards, give me your top 10 matching doc IDs"          │
│                                                               │
│     ┌─────────────────────────────────────────────────────┐  │
│     │                     SCATTER                          │  │
│     └─────────────────────────────────────────────────────┘  │
│           │              │              │                     │
│           ▼              ▼              ▼                     │
│      ┌─────────┐   ┌─────────┐   ┌─────────┐                 │
│      │ Shard 0 │   │ Shard 1 │   │ Shard 2 │                 │
│      │         │   │         │   │         │                 │
│      │ Returns:│   │ Returns:│   │ Returns:│                 │
│      │ doc1: 9 │   │ doc45: 8│   │ doc99: 10│                │
│      │ doc2: 7 │   │ doc46: 6│   │ doc100: 5│                │
│      │ ...     │   │ ...     │   │ ...      │                │
│      └─────────┘   └─────────┘   └─────────┘                 │
│                                                               │
│  Coordinator merges: doc99(10), doc1(9), doc45(8), doc2(7)..│
│  Global top 10 determined!                                    │
└──────────────────────────────────────────────────────────────┘
```

**Phase 2: Fetch (Gather)**
```
┌──────────────────────────────────────────────────────────────┐
│                        COORDINATOR                            │
│                                                               │
│  "OK, I need the full documents for: doc99, doc1, doc45..."  │
│                                                               │
│     Only contacts shards that have winning docs              │
│                                                               │
│        ┌─────────┐              ┌─────────┐                  │
│        │ Shard 0 │              │ Shard 2 │                  │
│        │         │              │         │                  │
│        │ Returns:│              │ Returns:│                  │
│        │ doc1:   │              │ doc99:  │                  │
│        │ {full   │              │ {full   │                  │
│        │  JSON}  │              │  JSON}  │                  │
│        └─────────┘              └─────────┘                  │
│                                                               │
│  Returns full results to client!                              │
└──────────────────────────────────────────────────────────────┘
```

### Deep Pagination Problem

```
Request: Give me page 1000, with 10 results per page

What the coordinator needs from each shard:
  10,010 results (to find global position 10001-10010)

With 5 shards:
  5 × 10,010 = 50,050 results to merge in memory!

With more pages:
  Page 10,000 × 5 shards × 10 results = 500,000 in memory!

→ Memory explosion! Cluster crashes!
```

**Solution: Use `search_after`**

```json
// First page
GET /products/_search
{
  "size": 10,
  "sort": [
    { "created_at": "desc" },
    { "_id": "asc" }
  ]
}

// Returns last result: { "sort": ["2024-01-15T10:30:00", "product_123"] }

// Next page (uses last result's sort values)
GET /products/_search
{
  "size": 10,
  "search_after": ["2024-01-15T10:30:00", "product_123"],
  "sort": [
    { "created_at": "desc" },
    { "_id": "asc" }
  ]
}

// Each shard only needs to return 10 results!
// No memory explosion!
```

---

## Shard Sizing Guide

### The Sweet Spot: 20-50 GB per Shard

| Shard Size | Status | Why |
|------------|--------|-----|
| < 1 GB | Over-sharded | Too much overhead per shard |
| 1-20 GB | Acceptable | OK for small indices |
| **20-50 GB** | **Ideal** | **Optimal balance** |
| 50-100 GB | Large | Longer recovery time |
| > 100 GB | Under-sharded | Very slow operations |

### How Many Shards Do I Need?

```python
def calculate_shards(data_size_gb, target_shard_size_gb=30):
    return max(1, ceil(data_size_gb / target_shard_size_gb))

# Examples:
calculate_shards(10)   # → 1 shard (small index)
calculate_shards(50)   # → 2 shards
calculate_shards(200)  # → 7 shards
calculate_shards(1000) # → 34 shards
```

### Over-Sharding Penalty

```
Index: 10 GB
Shards: 100

Problems:
1. 100 shards × ~500KB metadata = 50MB heap wasted
2. Query fans out to 100 shards (coordination overhead)
3. Each shard is only 100MB (inefficient)

Recommendation: 1 shard would be better!
```

### Under-Sharding Penalty

```
Index: 500 GB
Shards: 1

Problems:
1. Can't distribute across multiple nodes
2. Single point of failure
3. Recovery = copy entire 500GB (hours!)
4. Merges take forever

Recommendation: 10-15 shards would be better!
```

### ⚠️ Critical: Shard Count is Fixed!

```json
// Create index with 5 shards
PUT /products
{
  "settings": {
    "number_of_shards": 5  // CAN'T CHANGE THIS LATER!
  }
}

// Later, you realize you need more shards...
// You MUST reindex to a new index!

POST /_reindex
{
  "source": { "index": "products" },
  "dest": { "index": "products-v2" }  // With more shards
}
```

---

## Capacity Planning Example

### Requirements

```
- 100 million products
- Average document size: 2 KB
- Expected traffic: 1,000 queries/second
- P99 latency requirement: < 100ms
```

### Calculations

**Data Size:**
```
100M documents × 2 KB = 200 GB raw data
With index overhead (1.5x): ~300 GB index size
```

**Shard Count:**
```
300 GB ÷ 30 GB/shard = 10 shards
```

**Replica Count:**
```
1 replica for availability and read throughput
Total shard copies: 10 primary + 10 replica = 20
```

**Node Sizing:**
```
Each node should handle 5 shard copies comfortably.
Nodes needed: 20 ÷ 5 = 4 nodes

Per node:
- Heap: 30 GB (enough for 5 shards)
- Disk: 300 GB (75 GB per shard × 5 shards × 1.5 headroom)
- RAM: 64 GB (30 heap + 30 page cache + OS)
```

### Final Architecture

```
Cluster: 4 nodes
  - Node A: Shards 0, 1 (primary), 8, 9 (replica)
  - Node B: Shards 2, 3 (primary), 0, 1 (replica)
  - Node C: Shards 4, 5 (primary), 2, 3 (replica)
  - Node D: Shards 6, 7, 8, 9 (primary), 4, 5, 6, 7 (replica)

Each node: 64GB RAM, 300GB SSD, 16 cores
Total cost: ~$4,000/month (cloud)
```

---

## Key Takeaways

1. **Sharding splits data** across multiple machines for horizontal scaling.

2. **Routing determines** which shard gets each document: `hash(id) % shards`.

3. **Replicas provide** fault tolerance AND read throughput.

4. **Query is scatter-gather**: All shards searched, results merged.

5. **Deep pagination is dangerous** - use `search_after` instead.

6. **Shard count is fixed** - plan carefully or expect reindexing.

7. **Target 20-50 GB per shard** for optimal performance.

---

## Forward References

- **[Ch 4.3 Modeling](../data-foundation/4.3-modeling.md)**: How document modeling affects shard routing
- **[Ch 14 Distributed Architecture](../infra/14.1-distributed.md)**: Advanced multi-region patterns
