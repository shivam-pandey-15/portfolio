# 3.4 Vector Indices (HNSW)

## Plain English Summary

**Vector search finds things that are *semantically* similar, not just keyword matches.**

When you search for "affordable laptop for students", regular search looks for those exact words. But vector search understands that "cheap notebook for college" means the same thing, even without matching keywords.

How? Every piece of text gets converted into a list of numbers called a **vector** (or "embedding"). Similar meanings = similar numbers. The search engine then finds vectors that are "close" to your query's vector.

**The problem:** With millions of vectors, calculating every distance is too slow. HNSW (Hierarchical Navigable Small World) is a clever algorithm that finds approximate nearest neighbors very fast by organizing vectors into a navigable graph.

---

## Real-World Analogy: Finding Your Friend at a Concert

Imagine you're at a concert with 50,000 people and you need to find your friend.

**Brute Force (Exact Nearest Neighbor):**
```
Walk up to every single person, check if they're your friend.
50,000 people × 5 seconds = 3 days!
```

**Smart Search (HNSW Analogy):**
```
1. You know your friend is near the stage
   → Jump to the front section (skip 40,000 people)

2. Ask someone: "Have you seen someone matching this description?"
   → They point you to the left side

3. Ask another person in that area
   → They point you to a specific cluster of people

4. Walk through that small cluster, find your friend!
   → Only checked ~50 people instead of 50,000
```

HNSW works similarly: it creates a graph where vectors that are "close" (similar meanings) are connected. Instead of checking every vector, you navigate the graph to quickly find the neighborhood of similar vectors.

---

## Understanding Vectors (Embeddings)

### What's a Vector?

A vector is just a list of numbers that represents meaning:

```
"cat"     → [0.2, -0.5, 0.8, 0.1, ...]  (768 numbers)
"kitten"  → [0.3, -0.4, 0.7, 0.2, ...]  (very similar!)
"laptop"  → [0.9, 0.2, -0.3, 0.5, ...]  (very different!)
```

**Where do vectors come from?**

Neural networks (like BERT, sentence-transformers) are trained to produce similar vectors for similar meanings:

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')

vec1 = model.encode("affordable laptop for students")
vec2 = model.encode("cheap notebook for college")
vec3 = model.encode("expensive sports car")

similarity(vec1, vec2)  # → 0.92 (very similar!)
similarity(vec1, vec3)  # → 0.15 (very different!)
```

### Distance Metrics

**Cosine Similarity** (most common):
```
Two vectors pointing in similar directions = similar meaning

       vec1 →
         \
          \_____ angle = 15° → similar (cosine ≈ 0.96)
           \
            vec2 →

       vec1 →
         \
          \______ angle = 80° → different (cosine ≈ 0.17)
                  \
                   vec3 ↓
```

**Euclidean Distance** (L2):
```
Straight-line distance between vector endpoints.
Smaller distance = more similar.
```

---

## The Problem: Exact Search is Too Slow

### The Curse of Dimensionality

```python
# Query: Find 10 most similar products to "wireless keyboard"

query_vector = model.encode("wireless keyboard")  # 768 dimensions

# Brute force search
for each_product in all_products:  # 100 million products
    distance = calculate_distance(query_vector, product.vector)  # 768 operations
    store_in_heap(product, distance)

return heap.top(10)
```

**The math:**
```
100,000,000 products
× 768 dimensions
× 2 operations per dimension (subtract + square)
= 153,600,000,000 operations

At 1 nanosecond per operation = 153 seconds per query!
```

That's not acceptable. We need a smarter approach.

---

## HNSW: The Solution

### The Core Idea: Multi-Layer Navigation

HNSW builds a graph with multiple layers, like expressways and local roads:

```
Layer 3 (Express): [A] ────────────────────────────────── [Z]
                    │                                      │
Layer 2:           [A] ─────────── [M] ─────────────── [Z]
                    │               │                     │
Layer 1:           [A] ──── [F] ── [M] ──── [R] ──── [Z]
                    │        │      │        │        │
Layer 0 (Local):   [A][B][C][D][E][F][G][H][I][J]...[X][Y][Z]
                   (All vectors are in Layer 0)
```

**Why multiple layers?**
- **Layer 3**: Few connections, but they span across the entire space (like highways)
- **Layer 0**: Many connections, but only to nearby vectors (like local streets)

This lets you "zoom in" on the right neighborhood quickly.

### The Search Algorithm (Step by Step)

**Goal**: Find 10 nearest neighbors to query vector Q

```
Step 1: Start at the entry point in the top layer

         Layer 3:  [A] ────────────────── [Z]
                    ↑
                    Start here (entry point)
                    
         Query Q is close to Z

Step 2: Greedy search on Layer 3
         
         Check neighbors of A: [Z]
         Z is closer to Q than A
         Move to Z

Step 3: Can't improve on Layer 3 → Drop to Layer 2

         Layer 2:  [A] ──── [M] ──── [Z]
                                      ↑
                                      Current position
         
         Check neighbors of Z: [M]
         M is NOT closer to Q
         Stay at Z
         
         → Drop to Layer 1

Step 4: Continue this process through layers...

Step 5: At Layer 0, do a thorough local search

         Layer 0:  [...][V][W][X][Y][Z]
                              ↑  ↑  ↑
                              Explore neighborhood
                              
         Check all neighbors of Z
         For each neighbor, check their neighbors
         Continue until we've explored enough candidates
         
Step 6: Return top 10 closest vectors found
```

**Why this is fast:**
- Top layers: Long jumps across the space (skip millions of vectors)
- Bottom layer: Thorough search, but only in the right neighborhood
- Total vectors checked: ~150 (instead of 100 million!)

### Visual Example

```
Query: "wireless keyboard"

Vectors in index:
A: "gaming mouse"
B: "USB cable"
C: "wireless bluetooth keyboard"  ← Target!
D: "monitor stand"
E: "keyboard and mouse combo"     ← Also good
...
Z: "winter jacket"                ← Not relevant

HNSW search path:
1. Start at A (random entry)
2. Layer 3: A → Z (Z is closer to query)
3. Layer 2: Z → ... → M
4. Layer 1: M → ... → E
5. Layer 0: E → C (found nearest!)

Result: [C, E, ...] ranked by similarity
```

---

## HNSW Parameters (Tuning Guide)

### Construction Parameters

These are set when building the index:

| Parameter | What It Does | Default | Trade-off |
|-----------|--------------|---------|-----------|
| `M` | Max connections per node | 16 | Higher = better recall, more memory |
| `ef_construction` | Beam width during build | 100 | Higher = better graph quality, slower build |

**M explained:**
```
M = 16 means each vector connects to up to 16 neighbors

Low M (8):
    A ── B ── C ── D
    Sparse connections, might miss shortcuts
    
High M (32):
    A ─┬─ B ─┬─ C ─┬─ D
       │     │     │
       └─────┴─────┘
    Dense connections, better quality, 2x memory
```

### Search Parameters

These are set at query time:

| Parameter | What It Does | Default | Trade-off |
|-----------|--------------|---------|-----------|
| `ef` or `num_candidates` | Candidates to explore | 100 | Higher = better recall, slower search |

**ef explained:**
```
ef = 50:  Check 50 candidates → 92% recall, 2ms
ef = 100: Check 100 candidates → 96% recall, 4ms
ef = 200: Check 200 candidates → 99% recall, 8ms

"Recall" = What percentage of the TRUE top 10 did we actually find?
```

### Tuning Recommendations by Use Case

```
Use Case: Product Search (quality matters)
M = 32
ef_construction = 200
ef_search = 200
Expected: 99% recall, 10ms latency

Use Case: Log Analysis (speed matters)
M = 16
ef_construction = 100
ef_search = 50
Expected: 92% recall, 2ms latency

Use Case: Recommendation (balance)
M = 24
ef_construction = 150
ef_search = 100
Expected: 97% recall, 5ms latency
```

---

## Elasticsearch Implementation

### Creating a Vector Index

```json
PUT /products-semantic
{
  "mappings": {
    "properties": {
      "title": {
        "type": "text"
      },
      "title_vector": {
        "type": "dense_vector",
        "dims": 768,
        "index": true,
        "similarity": "cosine",
        "index_options": {
          "type": "hnsw",
          "m": 16,
          "ef_construction": 100
        }
      }
    }
  }
}
```

### Indexing Documents with Vectors

```json
POST /products-semantic/_doc
{
  "title": "Logitech Wireless Keyboard MX Keys",
  "title_vector": [0.12, -0.45, 0.89, ...]  // 768 floats from your model
}
```

**Important:** You must generate vectors outside Elasticsearch using a model like:
- sentence-transformers (Python)
- OpenAI embeddings API
- Cohere embed API

### Searching

```json
POST /products-semantic/_search
{
  "knn": {
    "field": "title_vector",
    "query_vector": [0.15, -0.42, 0.91, ...],  // Vector of user's query
    "k": 10,
    "num_candidates": 100  // ef_search parameter
  }
}
```

---

## Hybrid Search: Combining Keywords + Vectors

**The best of both worlds:**

```json
POST /products/_search
{
  "query": {
    "bool": {
      "should": [
        {
          "match": {
            "title": {
              "query": "wireless keyboard",
              "boost": 0.3  // Weight for keyword match
            }
          }
        }
      ],
      "filter": [
        { "term": { "category": "electronics" } }
      ]
    }
  },
  "knn": {
    "field": "title_vector",
    "query_vector": [0.15, -0.42, ...],
    "k": 50,
    "num_candidates": 100,
    "boost": 0.7  // Weight for semantic match
  }
}
```

**Score combination:**
```
final_score = 0.3 × BM25_score + 0.7 × vector_similarity_score
```

**When to use which weight:**
| Scenario | Keyword Weight | Vector Weight |
|----------|----------------|---------------|
| E-commerce (exact models matter) | 0.5 | 0.5 |
| FAQ search (meaning matters) | 0.2 | 0.8 |
| Code search (exact tokens matter) | 0.7 | 0.3 |
| General web search | 0.4 | 0.6 |

---

## Memory and Storage

### Vector Storage Requirements

```
Formula: size = num_docs × dimensions × bytes_per_float

Example:
- 10 million products
- 768 dimensions (BERT-based)
- 4 bytes per float (float32)

Storage = 10,000,000 × 768 × 4 = 30.7 GB

Plus HNSW graph (~50% overhead):
Total = ~46 GB
```

### Quantization (Memory Reduction)

**The idea:** Store less precise numbers to save space.

```
Original (float32): 3.141592653589793
                    Uses 4 bytes

Quantized (int8):   Scaled to 0-255 range
                    Uses 1 byte (4x smaller!)
                    
Trade-off: Slightly less accurate similarity calculations
```

**Impact:**
```
100 million vectors, 768 dims:

float32: 307 GB
int8:    77 GB (4x smaller)
Binary:  9.6 GB (32x smaller, significant quality loss)
```

### Elasticsearch int8 quantization (8.11+):

```json
{
  "mappings": {
    "properties": {
      "vector": {
        "type": "dense_vector",
        "dims": 768,
        "index": true,
        "index_options": {
          "type": "int8_hnsw"  // 4x memory reduction!
        }
      }
    }
  }
}
```

---

## Common Pitfalls

### Pitfall 1: Not Pre-filtering

```json
// BAD: Find top 10 shoes, then filter to category=shoes
{
  "knn": { "k": 10, ... },
  "post_filter": { "term": { "category": "shoes" } }
}

// Problem: Might return only 3 results!
// The 10 nearest vectors might include bags, shirts, etc.
// After filtering, only 3 are shoes.
```

```json
// GOOD: Filter FIRST, then find top 10 within shoes
{
  "knn": {
    "k": 10,
    "filter": { "term": { "category": "shoes" } },
    ...
  }
}

// Returns 10 shoes, as expected
```

### Pitfall 2: Stale Embeddings

```
Day 1: Index product "Blue Running Shoes $99"
       Vector captures "blue", "running", "shoes", "affordable"

Day 30: Update product to "Red Walking Shoes $299" (price doubled)
        Text is updated, but vector is NOT regenerated!
        
Search for "red walking shoes" → This product doesn't match well!
The vector still represents the OLD text.
```

**Solution:** Re-embed whenever text content changes.

### Pitfall 3: Wrong Dimensionality

```json
// Index expects 768 dimensions
{
  "mapping": {
    "vector": { "dims": 768 }
  }
}

// You send 384 dimensions
POST /_doc
{
  "vector": [0.1, 0.2, ..., 0.384]  // Only 384!
}

// Error! Dimensionality mismatch.
```

**Solution:** Always use the same model for indexing and querying.

---

## Performance Benchmarks

**Dataset: 10 million products, 768 dimensions**

| Configuration | Recall@10 | Latency (p50) | Latency (p99) |
|---------------|-----------|---------------|---------------|
| ef=50 | 92% | 2ms | 8ms |
| ef=100 | 96% | 4ms | 15ms |
| ef=200 | 98% | 8ms | 25ms |
| ef=500 | 99.5% | 20ms | 60ms |
| Brute force | 100% | 500ms | 800ms |

**Scaling behavior:**
```
10M vectors:  4ms at ef=100
100M vectors: 8ms at ef=100 (only 2x slower for 10x data!)
1B vectors:   15ms at ef=100 (logarithmic scaling)
```

---

## Key Takeaways

1. **Vectors capture meaning**, not just keywords. "affordable laptop" and "cheap notebook" have similar vectors.

2. **HNSW finds approximate neighbors fast** by organizing vectors in a navigable graph.

3. **Parameters matter**: M affects memory and quality, ef affects search speed and recall.

4. **Hybrid search is usually best**: Combine keyword (BM25) and vector (HNSW) for best results.

5. **Watch for stale embeddings**: Re-embed when content changes.

6. **Use quantization** to reduce memory by 4x with minimal quality loss.

---

## Forward References

- **[Ch 6 Vector Search](../vector-search/6.1-limitations.md)**: Building full semantic search systems
- **[Ch 7 Training Embeddings](../embedding-training/7.1-why.md)**: Creating domain-specific embedding models
